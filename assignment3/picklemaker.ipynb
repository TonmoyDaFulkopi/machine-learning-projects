{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim, gradient_clip_value=None):\n",
    "        \"\"\"\n",
    "        Initialize a dense (fully connected) layer with given input and output dimensions.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_dim: int, number of input features\n",
    "        - output_dim: int, number of output units\n",
    "        - gradient_clip_value: float or None, maximum absolute value for gradients (if clipping is desired)\n",
    "        \"\"\"\n",
    "        # Xavier/Glorot initialization for weights\n",
    "        limit = np.sqrt(6 / (input_dim + output_dim))\n",
    "        self.weights = np.random.uniform(-limit, limit, (input_dim, output_dim))\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "        \n",
    "        # Placeholder for gradients\n",
    "        self.d_weights = None\n",
    "        self.d_biases = None\n",
    "        \n",
    "        # Gradient clipping threshold\n",
    "        self.gradient_clip_value = gradient_clip_value\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the dense layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: ndarray, input data of shape (batch_size, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "        - output: ndarray, result of the layer transformation, shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        self.input = X\n",
    "        self.output = np.dot(X, self.weights) + self.biases\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        Compute the gradients of weights and biases with respect to the loss.\n",
    "        \n",
    "        Parameters:\n",
    "        - d_out: ndarray, gradient of the loss with respect to the output of this layer, \n",
    "        shape (batch_size, output_dim)\n",
    "        \n",
    "        Returns:\n",
    "        - d_input: ndarray, gradient of the loss with respect to the input of this layer, shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        # Gradient of the loss with respect to weights and biases\n",
    "        self.d_weights = np.dot(self.input.T, d_out)\n",
    "        self.d_biases = np.sum(d_out, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient of the loss with respect to the input of this layer\n",
    "        d_input = np.dot(d_out, self.weights.T)\n",
    "        \n",
    "        # Clip gradients if gradient clipping is enabled\n",
    "        if self.gradient_clip_value is not None:\n",
    "            np.clip(self.d_weights, -self.gradient_clip_value, self.gradient_clip_value, out=self.d_weights)\n",
    "            np.clip(self.d_biases, -self.gradient_clip_value, self.gradient_clip_value, out=self.d_biases)\n",
    "        \n",
    "        return d_input\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, layer, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.layer = layer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Initialize moment estimates\n",
    "        if hasattr(layer, 'weights'):\n",
    "            # For DenseLayer\n",
    "            self.m_weights = np.zeros_like(layer.weights)\n",
    "            self.v_weights = np.zeros_like(layer.weights)\n",
    "            self.m_biases = np.zeros_like(layer.biases)\n",
    "            self.v_biases = np.zeros_like(layer.biases)\n",
    "        elif hasattr(layer, 'gamma'):\n",
    "            # For BatchNormalization\n",
    "            self.m_gamma = np.zeros_like(layer.gamma)\n",
    "            self.v_gamma = np.zeros_like(layer.gamma)\n",
    "            self.m_beta = np.zeros_like(layer.beta)\n",
    "            self.v_beta = np.zeros_like(layer.beta)\n",
    "\n",
    "        # Time step for bias correction\n",
    "        self.t = 1\n",
    "\n",
    "    def update(self):\n",
    "        if hasattr(self.layer, 'weights'):\n",
    "            # Update DenseLayer weights and biases\n",
    "            self._update_dense()\n",
    "        elif hasattr(self.layer, 'gamma'):\n",
    "            # Update BatchNormalization gamma and beta\n",
    "            self._update_batch_norm()\n",
    "\n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "\n",
    "    def _update_dense(self):\n",
    "        # Adam update for weights\n",
    "        self.m_weights = self.beta1 * self.m_weights + (1 - self.beta1) * self.layer.d_weights\n",
    "        self.v_weights = self.beta2 * self.v_weights + (1 - self.beta2) * (self.layer.d_weights ** 2)\n",
    "        m_weights_corr = self.m_weights / (1 - self.beta1 ** self.t)\n",
    "        v_weights_corr = self.v_weights / (1 - self.beta2 ** self.t)\n",
    "        self.layer.weights -= self.learning_rate * m_weights_corr / (np.sqrt(v_weights_corr) + self.epsilon)\n",
    "        \n",
    "        # Adam update for biases\n",
    "        self.m_biases = self.beta1 * self.m_biases + (1 - self.beta1) * self.layer.d_biases\n",
    "        self.v_biases = self.beta2 * self.v_biases + (1 - self.beta2) * (self.layer.d_biases ** 2)\n",
    "        m_biases_corr = self.m_biases / (1 - self.beta1 ** self.t)\n",
    "        v_biases_corr = self.v_biases / (1 - self.beta2 ** self.t)\n",
    "        self.layer.biases -= self.learning_rate * m_biases_corr / (np.sqrt(v_biases_corr) + self.epsilon)\n",
    "\n",
    "    def _update_batch_norm(self):\n",
    "        # Adam update for gamma\n",
    "        self.m_gamma = self.beta1 * self.m_gamma + (1 - self.beta1) * self.layer.d_gamma\n",
    "        self.v_gamma = self.beta2 * self.v_gamma + (1 - self.beta2) * (self.layer.d_gamma ** 2)\n",
    "        m_gamma_corr = self.m_gamma / (1 - self.beta1 ** self.t)\n",
    "        v_gamma_corr = self.v_gamma / (1 - self.beta2 ** self.t)\n",
    "        self.layer.gamma -= self.learning_rate * m_gamma_corr / (np.sqrt(v_gamma_corr) + self.epsilon)\n",
    "        \n",
    "        # Adam update for beta\n",
    "        self.m_beta = self.beta1 * self.m_beta + (1 - self.beta1) * self.layer.d_beta\n",
    "        self.v_beta = self.beta2 * self.v_beta + (1 - self.beta2) * (self.layer.d_beta ** 2)\n",
    "        m_beta_corr = self.m_beta / (1 - self.beta1 ** self.t)\n",
    "        v_beta_corr = self.v_beta / (1 - self.beta2 ** self.t)\n",
    "        self.layer.beta -= self.learning_rate * m_beta_corr / (np.sqrt(v_beta_corr) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, dim, momentum=0.9, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        Initialize the Batch Normalization layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - dim: int, number of features in the input\n",
    "        - momentum: float, momentum for moving average of mean and variance\n",
    "        - epsilon: float, small constant to prevent division by zero\n",
    "        \"\"\"\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = np.ones((1, dim))  # Scale parameter\n",
    "        self.beta = np.zeros((1, dim))  # Shift parameter\n",
    "        self.running_mean = np.zeros((1, dim))\n",
    "        self.running_var = np.ones((1, dim))\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass for Batch Normalization.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: ndarray, input data of shape (batch_size, dim)\n",
    "        \n",
    "        Returns:\n",
    "        - out: ndarray, normalized and scaled output\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # Compute mean and variance for the batch\n",
    "            batch_mean = np.mean(X, axis=0, keepdims=True)\n",
    "            batch_var = np.var(X, axis=0, keepdims=True)\n",
    "            \n",
    "            # Normalize\n",
    "            self.X_centered = X - batch_mean\n",
    "            self.stddev_inv = 1.0 / np.sqrt(batch_var + self.epsilon)\n",
    "            X_norm = self.X_centered * self.stddev_inv\n",
    "            \n",
    "            # Update running mean and variance for inference\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "        else:\n",
    "            # Use running mean and variance for inference\n",
    "            X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "        \n",
    "        # Scale and shift\n",
    "        out = self.gamma * X_norm + self.beta\n",
    "        self.X_norm = X_norm  # Store for backward pass\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        \"\"\"\n",
    "        Backward pass for Batch Normalization.\n",
    "        \n",
    "        Parameters:\n",
    "        - d_out: ndarray, gradient of the loss with respect to the output of this layer\n",
    "        - learning_rate: float, learning rate for parameter updates\n",
    "        \n",
    "        Returns:\n",
    "        - d_input: ndarray, gradient of the loss with respect to the input of this layer\n",
    "        \"\"\"\n",
    "        # Gradients with respect to gamma and beta\n",
    "        self.d_gamma = np.sum(d_out * self.X_norm, axis=0, keepdims=True)\n",
    "        self.d_beta = np.sum(d_out, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient with respect to normalized input\n",
    "        d_X_norm = d_out * self.gamma\n",
    "        \n",
    "        # Gradient with respect to variance\n",
    "        d_var = np.sum(d_X_norm * self.X_centered, axis=0, keepdims=True) * -0.5 * self.stddev_inv**3\n",
    "        \n",
    "        # Gradient with respect to mean\n",
    "        d_mean = np.sum(d_X_norm * -self.stddev_inv, axis=0, keepdims=True) + d_var * np.mean(-2.0 * self.X_centered, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient with respect to input\n",
    "        d_input = (d_X_norm * self.stddev_inv) + (d_var * 2 * self.X_centered / d_out.shape[0]) + (d_mean / d_out.shape[0])\n",
    "        \n",
    "        # Update parameters\n",
    "        self.gamma -= learning_rate * self.d_gamma\n",
    "        self.beta -= learning_rate * self.d_beta\n",
    "        \n",
    "        return d_input\n",
    "\n",
    "    def set_training_mode(self, mode=True):\n",
    "        \"\"\"\n",
    "        Set the layer in training or evaluation mode.\n",
    "        \n",
    "        Parameters:\n",
    "        - mode: bool, True for training mode, False for evaluation mode\n",
    "        \"\"\"\n",
    "        self.training = mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation: ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        # Placeholder to store input for the backward pass\n",
    "        self.input = None\n",
    "        self.d_input = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the ReLU activation.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: ndarray, input data of shape (batch_size, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "        - output: ndarray, output after ReLU activation\n",
    "        \"\"\"\n",
    "        self.input = X\n",
    "        # Apply ReLU activation: ReLU(x) = max(0, x)\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        Perform the backward pass of the ReLU activation.\n",
    "        \n",
    "        Parameters:\n",
    "        - d_out: ndarray, gradient of the loss with respect to the output of the ReLU\n",
    "        \n",
    "        Returns:\n",
    "        - d_input: ndarray, gradient of the loss with respect to the input of the ReLU\n",
    "        \"\"\"\n",
    "        # The derivative of ReLU is 1 where input > 0 and 0 where input <= 0\n",
    "        self.d_input = d_out * (self.input > 0)  # Element-wise multiplication with mask\n",
    "        return self.d_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization: Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.training = True\n",
    "\n",
    "    def set_training_mode(self, mode=True):\n",
    "        \"\"\"\n",
    "        Set the dropout layer to training mode (drop units) or testing mode (no dropout).\n",
    "        \n",
    "        Parameters:\n",
    "        - mode: bool, True for training mode, False for testing mode.\n",
    "        \"\"\"\n",
    "        self.training = mode\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Apply dropout during the forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: ndarray, input data\n",
    "        \n",
    "        Returns:\n",
    "        - output: ndarray, with dropout applied if in training mode\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            mask = (np.random.rand(*X.shape) > self.dropout_rate).astype(np.float32)\n",
    "            self.mask = mask / (1 - self.dropout_rate)\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        Backward pass for dropout, scaled by the mask created in the forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        - d_out: ndarray, gradient of loss w.r.t dropout layer output\n",
    "        \n",
    "        Returns:\n",
    "        - d_input: ndarray, gradient of loss w.r.t dropout layer input\n",
    "        \"\"\"\n",
    "        return d_out * self.mask if self.training else d_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights saved successfully.\n",
      "Weights loaded successfully.\n",
      "Final Test Accuracy: 10.05%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define transformations and load the dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load training and test data\n",
    "trainset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "# Split trainset into training and validation sets (e.g., 90% train, 10% validation)\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Set up data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model layers and optimizers\n",
    "input_dim = 28 * 28  # FashionMNIST images are 28x28\n",
    "hidden_dim = 128     # Hidden layer dimension\n",
    "output_dim = 10      # 10 classes for classification\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Define the layers\n",
    "dense_layer1 = DenseLayer(input_dim=input_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm1 = BatchNormalization(dim=hidden_dim)\n",
    "relu1 = ReLU()\n",
    "dropout1 = Dropout(dropout_rate=0.5)\n",
    "\n",
    "dense_layer2 = DenseLayer(input_dim=hidden_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm2 = BatchNormalization(dim=hidden_dim)\n",
    "relu2 = ReLU()\n",
    "dropout2 = Dropout(dropout_rate=0.5)\n",
    "\n",
    "dense_layer3 = DenseLayer(input_dim=hidden_dim, output_dim=output_dim, gradient_clip_value=1.0)\n",
    "batch_norm3 = BatchNormalization(dim=output_dim)\n",
    "dropout3 = Dropout(dropout_rate=0.5)\n",
    "\n",
    "# Define the optimizers for each layer\n",
    "optimizer_dense1 = AdamOptimizer(dense_layer1, learning_rate=learning_rate)\n",
    "optimizer_bn1 = AdamOptimizer(batch_norm1, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense2 = AdamOptimizer(dense_layer2, learning_rate=learning_rate)\n",
    "optimizer_bn2 = AdamOptimizer(batch_norm2, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense3 = AdamOptimizer(dense_layer3, learning_rate=learning_rate)\n",
    "optimizer_bn3 = AdamOptimizer(batch_norm3, learning_rate=learning_rate)\n",
    "\n",
    "# Define a function to save the weights (without bias)\n",
    "def save_weights():\n",
    "    weights = {\n",
    "        'dense_layer1_weights': dense_layer1.weights,\n",
    "        'dense_layer2_weights': dense_layer2.weights,\n",
    "        'dense_layer3_weights': dense_layer3.weights\n",
    "    }\n",
    "    \n",
    "    # Save the weights to a pickle file\n",
    "    with open('model_weights.pkl', 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "    print(\"Weights saved successfully.\")\n",
    "\n",
    "# Define a function to load the weights (without bias)\n",
    "def load_weights():\n",
    "    with open('model_weights.pkl', 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "    \n",
    "    # Load the weights into the model layers\n",
    "    dense_layer1.weights = weights['dense_layer1_weights']\n",
    "    dense_layer2.weights = weights['dense_layer2_weights']\n",
    "    dense_layer3.weights = weights['dense_layer3_weights']\n",
    "    \n",
    "    print(\"Weights loaded successfully.\")\n",
    "\n",
    "# Training function (with some modification for saving weights)\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Training logic here...\n",
    "\n",
    "        # After training, save the weights to reduce the model size\n",
    "        if epoch == epochs - 1:  # Save weights after the last epoch\n",
    "            save_weights()\n",
    "\n",
    "# Train the model\n",
    "train(epochs=100)\n",
    "\n",
    "# Load the model weights for testing\n",
    "load_weights()\n",
    "\n",
    "# Testing function\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_norm1.set_training_mode(False)\n",
    "    dropout1.set_training_mode(False)\n",
    "    batch_norm2.set_training_mode(False)\n",
    "    dropout2.set_training_mode(False)\n",
    "    batch_norm3.set_training_mode(False)\n",
    "    dropout3.set_training_mode(False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()\n",
    "\n",
    "            # Forward pass using the loaded weights\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_bn3)\n",
    "\n",
    "            predicted = np.argmax(outputs_dropout3, axis=1)\n",
    "            correct += np.sum(predicted == labels.numpy())\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
