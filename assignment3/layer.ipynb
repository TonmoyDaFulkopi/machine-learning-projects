{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt4 made this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# # define transformation\n",
    "# transform = transforms.ToTensor()\n",
    "\n",
    "# # download and load the training data\n",
    "# trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "\n",
    "# # create a dataloader\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# # download and load the test data\n",
    "# testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "# # create a dataloader\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "# # define the network architecture\n",
    "# from torch import nn, optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(784, 256)\n",
    "#         self.fc2 = nn.Linear(256, 128)\n",
    "#         self.fc3 = nn.Linear(128, 64)\n",
    "#         self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # make sure input tensor is flattened\n",
    "#         x = x.view(x.shape[0], -1)\n",
    "        \n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "#         x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "#         return x\n",
    "    \n",
    "# model = Classifier()\n",
    "\n",
    "# # define the loss\n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "# # define the optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# # train the network\n",
    "# epochs = 5\n",
    "\n",
    "# for e in range(epochs):\n",
    "#     running_loss = 0\n",
    "#     for images, labels in trainloader:\n",
    "#         # clear the gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # forward pass\n",
    "#         output = model(images)\n",
    "#         loss = criterion(output, labels)\n",
    "        \n",
    "#         # backward pass\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "#     else:\n",
    "#         print(f\"Training loss: {running_loss/len(trainloader)}\")\n",
    "\n",
    "# # test the network\n",
    "# correct = 0\n",
    "# total = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for images, labels in testloader:\n",
    "#         output = model(images)\n",
    "#         _, predicted = torch.max(output, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "# print(f\"Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "\n",
    "# ### 2.2.2. Saving and loading models\n",
    "\n",
    "# # save the model\n",
    "# torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "# # load the model\n",
    "# state_dict = torch.load('checkpoint.pth')\n",
    "# print(state_dict.keys())\n",
    "\n",
    "# model.load_state_dict(state_dict)\n",
    "\n",
    "# # continue training\n",
    "# model = Classifier()\n",
    "# model.load_state_dict(state_dict)\n",
    "\n",
    "# # continue training\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# epochs = 1\n",
    "\n",
    "# for e in range(epochs):\n",
    "#     running_loss = 0\n",
    "#     for images, labels in trainloader:\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(images)\n",
    "#         loss = criterion(output, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#     else:\n",
    "#         print(f\"Training loss: {running_loss/len(trainloader)}\")\n",
    "\n",
    "\n",
    "# ### 2.2.3. Loading image data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denselayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim, gradient_clip_value=None):\n",
    "        \"\"\"\n",
    "        Initialize a dense (fully connected) layer with given input and output dimensions.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_dim: int, number of input features\n",
    "        - output_dim: int, number of output units\n",
    "        - gradient_clip_value: float or None, maximum absolute value for gradients (if clipping is desired)\n",
    "        \"\"\"\n",
    "        # Xavier/Glorot initialization for weights\n",
    "        limit = np.sqrt(6 / (input_dim + output_dim))\n",
    "        self.weights = np.random.uniform(-limit, limit, (input_dim, output_dim))\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "        \n",
    "        # Placeholder for gradients\n",
    "        self.d_weights = None\n",
    "        self.d_biases = None\n",
    "        \n",
    "        # Gradient clipping threshold\n",
    "        self.gradient_clip_value = gradient_clip_value\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the dense layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: ndarray, input data of shape (batch_size, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "        - output: ndarray, result of the layer transformation, shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        self.input = X\n",
    "        self.output = np.dot(X, self.weights) + self.biases\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        Compute the gradients of weights and biases with respect to the loss.\n",
    "        \n",
    "        Parameters:\n",
    "        - d_out: ndarray, gradient of the loss with respect to the output of this layer, \n",
    "        shape (batch_size, output_dim)\n",
    "        \n",
    "        Returns:\n",
    "        - d_input: ndarray, gradient of the loss with respect to the input of this layer, shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        # Gradient of the loss with respect to weights and biases\n",
    "        self.d_weights = np.dot(self.input.T, d_out)\n",
    "        self.d_biases = np.sum(d_out, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient of the loss with respect to the input of this layer\n",
    "        d_input = np.dot(d_out, self.weights.T)\n",
    "        \n",
    "        # Clip gradients if gradient clipping is enabled\n",
    "        if self.gradient_clip_value is not None:\n",
    "            np.clip(self.d_weights, -self.gradient_clip_value, self.gradient_clip_value, out=self.d_weights)\n",
    "            np.clip(self.d_biases, -self.gradient_clip_value, self.gradient_clip_value, out=self.d_biases)\n",
    "        \n",
    "        return d_input\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, layer, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.layer = layer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Initialize moment estimates\n",
    "        if hasattr(layer, 'weights'):\n",
    "            # For DenseLayer\n",
    "            self.m_weights = np.zeros_like(layer.weights)\n",
    "            self.v_weights = np.zeros_like(layer.weights)\n",
    "            self.m_biases = np.zeros_like(layer.biases)\n",
    "            self.v_biases = np.zeros_like(layer.biases)\n",
    "        elif hasattr(layer, 'gamma'):\n",
    "            # For BatchNormalization\n",
    "            self.m_gamma = np.zeros_like(layer.gamma)\n",
    "            self.v_gamma = np.zeros_like(layer.gamma)\n",
    "            self.m_beta = np.zeros_like(layer.beta)\n",
    "            self.v_beta = np.zeros_like(layer.beta)\n",
    "\n",
    "        # Time step for bias correction\n",
    "        self.t = 1\n",
    "\n",
    "    def update(self):\n",
    "        if hasattr(self.layer, 'weights'):\n",
    "            # Update DenseLayer weights and biases\n",
    "            self._update_dense()\n",
    "        elif hasattr(self.layer, 'gamma'):\n",
    "            # Update BatchNormalization gamma and beta\n",
    "            self._update_batch_norm()\n",
    "\n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "\n",
    "    def _update_dense(self):\n",
    "        # Adam update for weights\n",
    "        self.m_weights = self.beta1 * self.m_weights + (1 - self.beta1) * self.layer.d_weights\n",
    "        self.v_weights = self.beta2 * self.v_weights + (1 - self.beta2) * (self.layer.d_weights ** 2)\n",
    "        m_weights_corr = self.m_weights / (1 - self.beta1 ** self.t)\n",
    "        v_weights_corr = self.v_weights / (1 - self.beta2 ** self.t)\n",
    "        self.layer.weights -= self.learning_rate * m_weights_corr / (np.sqrt(v_weights_corr) + self.epsilon)\n",
    "        \n",
    "        # Adam update for biases\n",
    "        self.m_biases = self.beta1 * self.m_biases + (1 - self.beta1) * self.layer.d_biases\n",
    "        self.v_biases = self.beta2 * self.v_biases + (1 - self.beta2) * (self.layer.d_biases ** 2)\n",
    "        m_biases_corr = self.m_biases / (1 - self.beta1 ** self.t)\n",
    "        v_biases_corr = self.v_biases / (1 - self.beta2 ** self.t)\n",
    "        self.layer.biases -= self.learning_rate * m_biases_corr / (np.sqrt(v_biases_corr) + self.epsilon)\n",
    "\n",
    "    def _update_batch_norm(self):\n",
    "        # Adam update for gamma\n",
    "        self.m_gamma = self.beta1 * self.m_gamma + (1 - self.beta1) * self.layer.d_gamma\n",
    "        self.v_gamma = self.beta2 * self.v_gamma + (1 - self.beta2) * (self.layer.d_gamma ** 2)\n",
    "        m_gamma_corr = self.m_gamma / (1 - self.beta1 ** self.t)\n",
    "        v_gamma_corr = self.v_gamma / (1 - self.beta2 ** self.t)\n",
    "        self.layer.gamma -= self.learning_rate * m_gamma_corr / (np.sqrt(v_gamma_corr) + self.epsilon)\n",
    "        \n",
    "        # Adam update for beta\n",
    "        self.m_beta = self.beta1 * self.m_beta + (1 - self.beta1) * self.layer.d_beta\n",
    "        self.v_beta = self.beta2 * self.v_beta + (1 - self.beta2) * (self.layer.d_beta ** 2)\n",
    "        m_beta_corr = self.m_beta / (1 - self.beta1 ** self.t)\n",
    "        v_beta_corr = self.v_beta / (1 - self.beta2 ** self.t)\n",
    "        self.layer.beta -= self.learning_rate * m_beta_corr / (np.sqrt(v_beta_corr) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, dim, momentum=0.9, epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        Initialize the Batch Normalization layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - dim: int, number of features in the input\n",
    "        - momentum: float, momentum for moving average of mean and variance\n",
    "        - epsilon: float, small constant to prevent division by zero\n",
    "        \"\"\"\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = np.ones((1, dim))  # Scale parameter\n",
    "        self.beta = np.zeros((1, dim))  # Shift parameter\n",
    "        self.running_mean = np.zeros((1, dim))\n",
    "        self.running_var = np.ones((1, dim))\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass for Batch Normalization.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: ndarray, input data of shape (batch_size, dim)\n",
    "        \n",
    "        Returns:\n",
    "        - out: ndarray, normalized and scaled output\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # Compute mean and variance for the batch\n",
    "            batch_mean = np.mean(X, axis=0, keepdims=True)\n",
    "            batch_var = np.var(X, axis=0, keepdims=True)\n",
    "            \n",
    "            # Normalize\n",
    "            self.X_centered = X - batch_mean\n",
    "            self.stddev_inv = 1.0 / np.sqrt(batch_var + self.epsilon)\n",
    "            X_norm = self.X_centered * self.stddev_inv\n",
    "            \n",
    "            # Update running mean and variance for inference\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "        else:\n",
    "            # Use running mean and variance for inference\n",
    "            X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "        \n",
    "        # Scale and shift\n",
    "        out = self.gamma * X_norm + self.beta\n",
    "        self.X_norm = X_norm  # Store for backward pass\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        \"\"\"\n",
    "        Backward pass for Batch Normalization.\n",
    "        \n",
    "        Parameters:\n",
    "        - d_out: ndarray, gradient of the loss with respect to the output of this layer\n",
    "        - learning_rate: float, learning rate for parameter updates\n",
    "        \n",
    "        Returns:\n",
    "        - d_input: ndarray, gradient of the loss with respect to the input of this layer\n",
    "        \"\"\"\n",
    "        # Gradients with respect to gamma and beta\n",
    "        self.d_gamma = np.sum(d_out * self.X_norm, axis=0, keepdims=True)\n",
    "        self.d_beta = np.sum(d_out, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient with respect to normalized input\n",
    "        d_X_norm = d_out * self.gamma\n",
    "        \n",
    "        # Gradient with respect to variance\n",
    "        d_var = np.sum(d_X_norm * self.X_centered, axis=0, keepdims=True) * -0.5 * self.stddev_inv**3\n",
    "        \n",
    "        # Gradient with respect to mean\n",
    "        d_mean = np.sum(d_X_norm * -self.stddev_inv, axis=0, keepdims=True) + d_var * np.mean(-2.0 * self.X_centered, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient with respect to input\n",
    "        d_input = (d_X_norm * self.stddev_inv) + (d_var * 2 * self.X_centered / d_out.shape[0]) + (d_mean / d_out.shape[0])\n",
    "        \n",
    "        # Update parameters\n",
    "        self.gamma -= learning_rate * self.d_gamma\n",
    "        self.beta -= learning_rate * self.d_beta\n",
    "        \n",
    "        return d_input\n",
    "\n",
    "    def set_training_mode(self, mode=True):\n",
    "        \"\"\"\n",
    "        Set the layer in training or evaluation mode.\n",
    "        \n",
    "        Parameters:\n",
    "        - mode: bool, True for training mode, False for evaluation mode\n",
    "        \"\"\"\n",
    "        self.training = mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation: ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        # Placeholder to store input for the backward pass\n",
    "        self.input = None\n",
    "        self.d_input = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the ReLU activation.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: ndarray, input data of shape (batch_size, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "        - output: ndarray, output after ReLU activation\n",
    "        \"\"\"\n",
    "        self.input = X\n",
    "        # Apply ReLU activation: ReLU(x) = max(0, x)\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        Perform the backward pass of the ReLU activation.\n",
    "        \n",
    "        Parameters:\n",
    "        - d_out: ndarray, gradient of the loss with respect to the output of the ReLU\n",
    "        \n",
    "        Returns:\n",
    "        - d_input: ndarray, gradient of the loss with respect to the input of the ReLU\n",
    "        \"\"\"\n",
    "        # The derivative of ReLU is 1 where input > 0 and 0 where input <= 0\n",
    "        self.d_input = d_out * (self.input > 0)  # Element-wise multiplication with mask\n",
    "        return self.d_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization: Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.training = True\n",
    "\n",
    "    def set_training_mode(self, mode=True):\n",
    "        \"\"\"\n",
    "        Set the dropout layer to training mode (drop units) or testing mode (no dropout).\n",
    "        \n",
    "        Parameters:\n",
    "        - mode: bool, True for training mode, False for testing mode.\n",
    "        \"\"\"\n",
    "        self.training = mode\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Apply dropout during the forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: ndarray, input data\n",
    "        \n",
    "        Returns:\n",
    "        - output: ndarray, with dropout applied if in training mode\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            mask = (np.random.rand(*X.shape) > self.dropout_rate).astype(np.float32)\n",
    "            self.mask = mask / (1 - self.dropout_rate)\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        \"\"\"\n",
    "        Backward pass for dropout, scaled by the mask created in the forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        - d_out: ndarray, gradient of loss w.r.t dropout layer output\n",
    "        \n",
    "        Returns:\n",
    "        - d_input: ndarray, gradient of loss w.r.t dropout layer input\n",
    "        \"\"\"\n",
    "        return d_out * self.mask if self.training else d_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Dense Layer + Batch NOrmalization(83%) + ReLU(77%) + Dropout 83.87%\n",
    "\n",
    "### lower learning rate, reordered layers, and dropout rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUNNABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 2.0474, Train Acc: 31.37%, Val Loss: 1.1562, Val Acc: 76.47%, Val Macro-F1: 0.7642\n",
      "Epoch 2/30, Train Loss: 1.6948, Train Acc: 40.01%, Val Loss: 1.0538, Val Acc: 79.03%, Val Macro-F1: 0.7899\n",
      "Epoch 3/30, Train Loss: 1.5963, Train Acc: 42.33%, Val Loss: 0.9981, Val Acc: 80.62%, Val Macro-F1: 0.8061\n",
      "Epoch 4/30, Train Loss: 1.5359, Train Acc: 44.14%, Val Loss: 0.9477, Val Acc: 81.47%, Val Macro-F1: 0.8156\n",
      "Epoch 5/30, Train Loss: 1.5002, Train Acc: 44.71%, Val Loss: 0.9280, Val Acc: 82.72%, Val Macro-F1: 0.8275\n",
      "Epoch 6/30, Train Loss: 1.4612, Train Acc: 45.92%, Val Loss: 0.8858, Val Acc: 83.30%, Val Macro-F1: 0.8333\n",
      "Epoch 7/30, Train Loss: 1.4352, Train Acc: 46.55%, Val Loss: 0.8580, Val Acc: 83.80%, Val Macro-F1: 0.8394\n",
      "Epoch 8/30, Train Loss: 1.4029, Train Acc: 47.64%, Val Loss: 0.8188, Val Acc: 84.18%, Val Macro-F1: 0.8414\n",
      "Epoch 9/30, Train Loss: 1.3976, Train Acc: 47.15%, Val Loss: 0.8172, Val Acc: 84.52%, Val Macro-F1: 0.8463\n",
      "Epoch 10/30, Train Loss: 1.3830, Train Acc: 47.92%, Val Loss: 0.7609, Val Acc: 84.82%, Val Macro-F1: 0.8474\n",
      "Epoch 11/30, Train Loss: 1.3757, Train Acc: 47.87%, Val Loss: 0.7653, Val Acc: 84.68%, Val Macro-F1: 0.8479\n",
      "Epoch 12/30, Train Loss: 1.3547, Train Acc: 48.48%, Val Loss: 0.7430, Val Acc: 85.23%, Val Macro-F1: 0.8529\n",
      "Epoch 13/30, Train Loss: 1.3470, Train Acc: 49.02%, Val Loss: 0.7438, Val Acc: 85.40%, Val Macro-F1: 0.8545\n",
      "Epoch 14/30, Train Loss: 1.3367, Train Acc: 49.12%, Val Loss: 0.7164, Val Acc: 85.80%, Val Macro-F1: 0.8588\n",
      "Epoch 15/30, Train Loss: 1.3200, Train Acc: 49.60%, Val Loss: 0.6975, Val Acc: 85.62%, Val Macro-F1: 0.8579\n",
      "Epoch 16/30, Train Loss: 1.3045, Train Acc: 50.08%, Val Loss: 0.6814, Val Acc: 85.95%, Val Macro-F1: 0.8604\n",
      "Epoch 17/30, Train Loss: 1.3024, Train Acc: 50.21%, Val Loss: 0.6768, Val Acc: 85.80%, Val Macro-F1: 0.8590\n",
      "Epoch 18/30, Train Loss: 1.3008, Train Acc: 49.89%, Val Loss: 0.6682, Val Acc: 85.97%, Val Macro-F1: 0.8606\n",
      "Epoch 19/30, Train Loss: 1.2808, Train Acc: 50.43%, Val Loss: 0.6552, Val Acc: 86.02%, Val Macro-F1: 0.8601\n",
      "Epoch 20/30, Train Loss: 1.2882, Train Acc: 50.11%, Val Loss: 0.6409, Val Acc: 86.25%, Val Macro-F1: 0.8632\n",
      "Epoch 21/30, Train Loss: 1.2796, Train Acc: 50.50%, Val Loss: 0.6384, Val Acc: 86.23%, Val Macro-F1: 0.8625\n",
      "Epoch 22/30, Train Loss: 1.2750, Train Acc: 50.37%, Val Loss: 0.6245, Val Acc: 86.42%, Val Macro-F1: 0.8639\n",
      "Epoch 23/30, Train Loss: 1.2693, Train Acc: 50.76%, Val Loss: 0.6139, Val Acc: 86.70%, Val Macro-F1: 0.8665\n",
      "Epoch 24/30, Train Loss: 1.2695, Train Acc: 50.40%, Val Loss: 0.6061, Val Acc: 86.72%, Val Macro-F1: 0.8675\n",
      "Epoch 25/30, Train Loss: 1.2594, Train Acc: 50.93%, Val Loss: 0.6075, Val Acc: 86.70%, Val Macro-F1: 0.8685\n",
      "Epoch 26/30, Train Loss: 1.2542, Train Acc: 50.93%, Val Loss: 0.5918, Val Acc: 86.78%, Val Macro-F1: 0.8684\n",
      "Epoch 27/30, Train Loss: 1.2537, Train Acc: 51.03%, Val Loss: 0.5858, Val Acc: 86.80%, Val Macro-F1: 0.8687\n",
      "Epoch 28/30, Train Loss: 1.2520, Train Acc: 51.06%, Val Loss: 0.5728, Val Acc: 86.97%, Val Macro-F1: 0.8701\n",
      "Epoch 29/30, Train Loss: 1.2467, Train Acc: 51.35%, Val Loss: 0.5913, Val Acc: 86.85%, Val Macro-F1: 0.8700\n",
      "Epoch 30/30, Train Loss: 1.2406, Train Acc: 51.31%, Val Loss: 0.5733, Val Acc: 86.78%, Val Macro-F1: 0.8687\n",
      "Final Test Accuracy: 23.82%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the DenseLayer, BatchNormalization, ReLU, Dropout, and AdamOptimizer classes as discussed above\n",
    "\n",
    "# Define transformations and load the dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load training and test data\n",
    "trainset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "# testset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "with open('b1.pkl', 'rb') as b1:\n",
    "  testset = pickle.load(b1)\n",
    "\n",
    "# Split trainset into training and validation sets (e.g., 90% train, 10% validation)\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Set up data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model layers and optimizers\n",
    "input_dim = 28 * 28  # FashionMNIST images are 28x28\n",
    "hidden_dim = 128     # Hidden layer dimension\n",
    "output_dim = 10      # 10 classes for classification\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Define the layers\n",
    "dense_layer1 = DenseLayer(input_dim=input_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm1 = BatchNormalization(dim=hidden_dim)\n",
    "relu1 = ReLU()  # Adding ReLU activation for first hidden layer\n",
    "dropout1 = Dropout(dropout_rate=0.5)  # Dropout for first hidden layer\n",
    "\n",
    "# Second hidden layer\n",
    "dense_layer2 = DenseLayer(input_dim=hidden_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm2 = BatchNormalization(dim=hidden_dim)\n",
    "relu2 = ReLU()  # Adding ReLU activation for second hidden layer\n",
    "dropout2 = Dropout(dropout_rate=0.5)  # Dropout for second hidden layer\n",
    "\n",
    "# Output layer\n",
    "dense_layer3 = DenseLayer(input_dim=hidden_dim, output_dim=output_dim, gradient_clip_value=1.0)\n",
    "batch_norm3 = BatchNormalization(dim=output_dim)\n",
    "dropout3 = Dropout(dropout_rate=0.5)  # Dropout for output layer\n",
    "\n",
    "# Define the optimizers for each layer\n",
    "optimizer_dense1 = AdamOptimizer(dense_layer1, learning_rate=learning_rate)\n",
    "optimizer_bn1 = AdamOptimizer(batch_norm1, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense2 = AdamOptimizer(dense_layer2, learning_rate=learning_rate)\n",
    "optimizer_bn2 = AdamOptimizer(batch_norm2, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense3 = AdamOptimizer(dense_layer3, learning_rate=learning_rate)\n",
    "optimizer_bn3 = AdamOptimizer(batch_norm3, learning_rate=learning_rate)\n",
    "\n",
    "# Define metrics storage for visualization\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "val_macro_f1s = []\n",
    "\n",
    "# Define a training function with Batch Normalization, Dropout, and hidden layers\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Set training mode\n",
    "        batch_norm1.set_training_mode(True)\n",
    "        dropout1.set_training_mode(True)\n",
    "        batch_norm2.set_training_mode(True)\n",
    "        dropout2.set_training_mode(True)\n",
    "        batch_norm3.set_training_mode(True)\n",
    "        dropout3.set_training_mode(True)\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in trainloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_bn3)\n",
    "\n",
    "            exp_outputs = np.exp(outputs_dropout3 - np.max(outputs_dropout3, axis=1, keepdims=True))\n",
    "            probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "            one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "            loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "            total_train_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            d_out = (probs - one_hot_labels) / labels.shape[0]\n",
    "            d_dropout3 = dropout3.backward(d_out)\n",
    "            d_bn3 = batch_norm3.backward(d_dropout3, learning_rate)\n",
    "            d_dense3 = dense_layer3.backward(d_bn3)\n",
    "\n",
    "            d_dropout2 = dropout2.backward(d_dense3)\n",
    "            d_relu2 = relu2.backward(d_dropout2)\n",
    "            d_bn2 = batch_norm2.backward(d_relu2, learning_rate)\n",
    "            d_dense2 = dense_layer2.backward(d_bn2)\n",
    "\n",
    "            d_dropout1 = dropout1.backward(d_dense2)\n",
    "            d_relu1 = relu1.backward(d_dropout1)\n",
    "            d_bn1 = batch_norm1.backward(d_relu1, learning_rate)\n",
    "            dense_layer1.backward(d_bn1)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer_dense1.update()\n",
    "            optimizer_bn1.update()\n",
    "            optimizer_dense2.update()\n",
    "            optimizer_bn2.update()\n",
    "            optimizer_dense3.update()\n",
    "            optimizer_bn3.update()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            predicted = np.argmax(probs, axis=1)\n",
    "            total_train_correct += np.sum(predicted == labels.numpy())\n",
    "            total_train_samples += labels.shape[0]\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "        train_losses.append(total_train_loss / len(trainloader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        batch_norm1.set_training_mode(False)\n",
    "        dropout1.set_training_mode(False)\n",
    "        batch_norm2.set_training_mode(False)\n",
    "        dropout2.set_training_mode(False)\n",
    "        batch_norm3.set_training_mode(False)\n",
    "        dropout3.set_training_mode(False)\n",
    "\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images = images.view(images.shape[0], -1).numpy()\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                outputs_dense1 = dense_layer1.forward(images)\n",
    "                outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "                outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "                outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "                outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "                outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "                outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "                outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "                outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "                outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "                outputs_dropout3 = dropout3.forward(outputs_bn3)\n",
    "\n",
    "                exp_outputs = np.exp(outputs_dropout3 - np.max(outputs_dropout3, axis=1, keepdims=True))\n",
    "                probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "                one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "                loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "                val_loss += loss\n",
    "\n",
    "                predicted = np.argmax(probs, axis=1)\n",
    "                val_correct += np.sum(predicted == labels.numpy())\n",
    "                val_total += labels.size(0)\n",
    "                all_preds.extend(predicted)\n",
    "                all_labels.extend(labels.numpy())\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        val_losses.append(val_loss / len(valloader))\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_macro_f1s.append(val_macro_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%, Val Macro-F1: {val_macro_f1s[-1]:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train(epochs=30)\n",
    "\n",
    "# Test function to evaluate on the test set\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_norm1.set_training_mode(False)\n",
    "    dropout1.set_training_mode(False)\n",
    "    batch_norm2.set_training_mode(False)\n",
    "    dropout2.set_training_mode(False)\n",
    "    batch_norm3.set_training_mode(False)\n",
    "    dropout3.set_training_mode(False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()\n",
    "\n",
    "            # Forward pass for testing\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_bn3)\n",
    "\n",
    "            predicted = np.argmax(outputs_dropout3, axis=1)\n",
    "            correct += np.sum(predicted == labels.numpy())\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test()\n",
    "\n",
    "# Plotting the metrics for 30 epochs\n",
    "epochs_range = range(1, 31)  # Now covers 1 to 30 epochs\n",
    "plt.figure(figsize=(14, 8))   # Optional: Increase figure width if desired\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation macro-F1 score\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs_range, val_macro_f1s, label=\"Validation Macro-F1 Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Macro-F1 Score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the DenseLayer, BatchNormalization, ReLU, Dropout, and AdamOptimizer classes as discussed above\n",
    "\n",
    "# Define transformations and load the dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load training and test data\n",
    "trainset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "# Split trainset into training and validation sets (e.g., 90% train, 10% validation)\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Set up data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model layers and optimizers\n",
    "input_dim = 28 * 28  # FashionMNIST images are 28x28\n",
    "hidden_dim = 128     # Hidden layer dimension\n",
    "output_dim = 10      # 10 classes for classification\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define the layers\n",
    "dense_layer1 = DenseLayer(input_dim=input_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm1 = BatchNormalization(dim=hidden_dim)\n",
    "relu = ReLU()  # Adding ReLU activation for hidden layer\n",
    "dropout1 = Dropout(dropout_rate=0.5)  # Dropout for hidden layer\n",
    "\n",
    "dense_layer2 = DenseLayer(input_dim=hidden_dim, output_dim=output_dim, gradient_clip_value=1.0)\n",
    "batch_norm2 = BatchNormalization(dim=output_dim)\n",
    "dropout2 = Dropout(dropout_rate=0.5)  # Dropout for output layer\n",
    "\n",
    "# Define the optimizers for each layer\n",
    "optimizer_dense1 = AdamOptimizer(dense_layer1, learning_rate=learning_rate)\n",
    "optimizer_bn1 = AdamOptimizer(batch_norm1, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense2 = AdamOptimizer(dense_layer2, learning_rate=learning_rate)\n",
    "optimizer_bn2 = AdamOptimizer(batch_norm2, learning_rate=learning_rate)\n",
    "\n",
    "# Define metrics storage for visualization\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "val_macro_f1s = []\n",
    "\n",
    "# Define a training function with Batch Normalization, Dropout, and a hidden layer\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Set training mode\n",
    "        batch_norm1.set_training_mode(True)\n",
    "        dropout1.set_training_mode(True)\n",
    "        batch_norm2.set_training_mode(True)\n",
    "        dropout2.set_training_mode(True)\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in trainloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu = relu.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_bn2)\n",
    "\n",
    "            exp_outputs = np.exp(outputs_dropout2 - np.max(outputs_dropout2, axis=1, keepdims=True))\n",
    "            probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "            one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "            loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "            total_train_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            d_out = (probs - one_hot_labels) / labels.shape[0]\n",
    "            d_dropout2 = dropout2.backward(d_out)\n",
    "            d_bn2 = batch_norm2.backward(d_dropout2, learning_rate)\n",
    "            d_dense2 = dense_layer2.backward(d_bn2)\n",
    "\n",
    "            d_dropout1 = dropout1.backward(d_dense2)\n",
    "            d_relu = relu.backward(d_dropout1)\n",
    "            d_bn1 = batch_norm1.backward(d_relu, learning_rate)\n",
    "            dense_layer1.backward(d_bn1)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer_dense1.update()\n",
    "            optimizer_bn1.update()\n",
    "            optimizer_dense2.update()\n",
    "            optimizer_bn2.update()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            predicted = np.argmax(probs, axis=1)\n",
    "            total_train_correct += np.sum(predicted == labels.numpy())\n",
    "            total_train_samples += labels.shape[0]\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "        train_losses.append(total_train_loss / len(trainloader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        batch_norm1.set_training_mode(False)\n",
    "        dropout1.set_training_mode(False)\n",
    "        batch_norm2.set_training_mode(False)\n",
    "        dropout2.set_training_mode(False)\n",
    "\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images = images.view(images.shape[0], -1).numpy()\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                outputs_dense1 = dense_layer1.forward(images)\n",
    "                outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "                outputs_relu = relu.forward(outputs_bn1)\n",
    "                outputs_dropout1 = dropout1.forward(outputs_relu)\n",
    "\n",
    "                outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "                outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "                outputs_dropout2 = dropout2.forward(outputs_bn2)\n",
    "\n",
    "                exp_outputs = np.exp(outputs_dropout2 - np.max(outputs_dropout2, axis=1, keepdims=True))\n",
    "                probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "                one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "                loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "                val_loss += loss\n",
    "\n",
    "                predicted = np.argmax(probs, axis=1)\n",
    "                val_correct += np.sum(predicted == labels.numpy())\n",
    "                val_total += labels.size(0)\n",
    "                all_preds.extend(predicted)\n",
    "                all_labels.extend(labels.numpy())\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        val_losses.append(val_loss / len(valloader))\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_macro_f1s.append(val_macro_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%, Val Macro-F1: {val_macro_f1s[-1]:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train(epochs=100)\n",
    "\n",
    "# Test function to evaluate on the test set\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_norm1.set_training_mode(False)\n",
    "    dropout1.set_training_mode(False)\n",
    "    batch_norm2.set_training_mode(False)\n",
    "    dropout2.set_training_mode(False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()\n",
    "\n",
    "            # Forward pass for testing\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu = relu.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_bn2)\n",
    "\n",
    "            predicted = np.argmax(outputs_dropout2, axis=1)\n",
    "            correct += np.sum(predicted == labels.numpy())\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test()\n",
    "\n",
    "# Plotting the metrics for 100 epochs\n",
    "epochs_range = range(1, 101)  # Now covers 1 to 100 epochs\n",
    "plt.figure(figsize=(14, 8))   # Optional: Increase figure width if desired\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation macro-F1 score\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs_range, val_macro_f1s, label=\"Validation Macro-F1 Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Macro-F1 Score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the DenseLayer, BatchNormalization, ReLU, Dropout, and AdamOptimizer classes as discussed above\n",
    "\n",
    "# Define transformations and load the dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load training and test data\n",
    "trainset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "# Split trainset into training and validation sets (e.g., 90% train, 10% validation)\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Set up data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model layers and optimizers\n",
    "input_dim = 28 * 28  # FashionMNIST images are 28x28\n",
    "hidden_dim = 128     # Hidden layer dimension\n",
    "output_dim = 10      # 10 classes for classification\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Define the layers\n",
    "dense_layer1 = DenseLayer(input_dim=input_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm1 = BatchNormalization(dim=hidden_dim)\n",
    "relu = ReLU()  # Adding ReLU activation for hidden layer\n",
    "dropout1 = Dropout(dropout_rate=0.5)  # Dropout for hidden layer\n",
    "\n",
    "dense_layer2 = DenseLayer(input_dim=hidden_dim, output_dim=output_dim, gradient_clip_value=1.0)\n",
    "batch_norm2 = BatchNormalization(dim=output_dim)\n",
    "dropout2 = Dropout(dropout_rate=0.5)  # Dropout for output layer\n",
    "\n",
    "# Define the optimizers for each layer\n",
    "optimizer_dense1 = AdamOptimizer(dense_layer1, learning_rate=learning_rate)\n",
    "optimizer_bn1 = AdamOptimizer(batch_norm1, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense2 = AdamOptimizer(dense_layer2, learning_rate=learning_rate)\n",
    "optimizer_bn2 = AdamOptimizer(batch_norm2, learning_rate=learning_rate)\n",
    "\n",
    "# Define metrics storage for visualization\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "val_macro_f1s = []\n",
    "\n",
    "# Define a training function with Batch Normalization, Dropout, and a hidden layer\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Set training mode\n",
    "        batch_norm1.set_training_mode(True)\n",
    "        dropout1.set_training_mode(True)\n",
    "        batch_norm2.set_training_mode(True)\n",
    "        dropout2.set_training_mode(True)\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in trainloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu = relu.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_bn2)\n",
    "\n",
    "            exp_outputs = np.exp(outputs_dropout2 - np.max(outputs_dropout2, axis=1, keepdims=True))\n",
    "            probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "            one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "            loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "            total_train_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            d_out = (probs - one_hot_labels) / labels.shape[0]\n",
    "            d_dropout2 = dropout2.backward(d_out)\n",
    "            d_bn2 = batch_norm2.backward(d_dropout2, learning_rate)\n",
    "            d_dense2 = dense_layer2.backward(d_bn2)\n",
    "\n",
    "            d_dropout1 = dropout1.backward(d_dense2)\n",
    "            d_relu = relu.backward(d_dropout1)\n",
    "            d_bn1 = batch_norm1.backward(d_relu, learning_rate)\n",
    "            dense_layer1.backward(d_bn1)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer_dense1.update()\n",
    "            optimizer_bn1.update()\n",
    "            optimizer_dense2.update()\n",
    "            optimizer_bn2.update()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            predicted = np.argmax(probs, axis=1)\n",
    "            total_train_correct += np.sum(predicted == labels.numpy())\n",
    "            total_train_samples += labels.shape[0]\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "        train_losses.append(total_train_loss / len(trainloader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        batch_norm1.set_training_mode(False)\n",
    "        dropout1.set_training_mode(False)\n",
    "        batch_norm2.set_training_mode(False)\n",
    "        dropout2.set_training_mode(False)\n",
    "\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images = images.view(images.shape[0], -1).numpy()\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                outputs_dense1 = dense_layer1.forward(images)\n",
    "                outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "                outputs_relu = relu.forward(outputs_bn1)\n",
    "                outputs_dropout1 = dropout1.forward(outputs_relu)\n",
    "\n",
    "                outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "                outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "                outputs_dropout2 = dropout2.forward(outputs_bn2)\n",
    "\n",
    "                exp_outputs = np.exp(outputs_dropout2 - np.max(outputs_dropout2, axis=1, keepdims=True))\n",
    "                probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "                one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "                loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "                val_loss += loss\n",
    "\n",
    "                predicted = np.argmax(probs, axis=1)\n",
    "                val_correct += np.sum(predicted == labels.numpy())\n",
    "                val_total += labels.size(0)\n",
    "                all_preds.extend(predicted)\n",
    "                all_labels.extend(labels.numpy())\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        val_losses.append(val_loss / len(valloader))\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_macro_f1s.append(val_macro_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%, Val Macro-F1: {val_macro_f1s[-1]:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train(epochs=100)\n",
    "\n",
    "# Test function to evaluate on the test set\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_norm1.set_training_mode(False)\n",
    "    dropout1.set_training_mode(False)\n",
    "    batch_norm2.set_training_mode(False)\n",
    "    dropout2.set_training_mode(False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()\n",
    "\n",
    "            # Forward pass for testing\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu = relu.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_bn2)\n",
    "\n",
    "            predicted = np.argmax(outputs_dropout2, axis=1)\n",
    "            correct += np.sum(predicted == labels.numpy())\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test()\n",
    "\n",
    "# Plotting the metrics for 100 epochs\n",
    "epochs_range = range(1, 101)  # Now covers 1 to 100 epochs\n",
    "plt.figure(figsize=(14, 8))   # Optional: Increase figure width if desired\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation macro-F1 score\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs_range, val_macro_f1s, label=\"Validation Macro-F1 Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Macro-F1 Score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the DenseLayer, BatchNormalization, ReLU, Dropout, and AdamOptimizer classes as discussed above\n",
    "\n",
    "# Define transformations and load the dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load training and test data\n",
    "trainset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "# Split trainset into training and validation sets (e.g., 90% train, 10% validation)\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Set up data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model layers and optimizers\n",
    "input_dim = 28 * 28  # FashionMNIST images are 28x28\n",
    "hidden_dim = 128     # Hidden layer dimension\n",
    "output_dim = 10      # 10 classes for classification\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Define the layers\n",
    "dense_layer1 = DenseLayer(input_dim=input_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm1 = BatchNormalization(dim=hidden_dim)\n",
    "relu = ReLU()  # Adding ReLU activation for hidden layer\n",
    "dropout1 = Dropout(dropout_rate=0.5)  # Dropout for hidden layer\n",
    "\n",
    "dense_layer2 = DenseLayer(input_dim=hidden_dim, output_dim=output_dim, gradient_clip_value=1.0)\n",
    "batch_norm2 = BatchNormalization(dim=output_dim)\n",
    "dropout2 = Dropout(dropout_rate=0.5)  # Dropout for output layer\n",
    "\n",
    "# Define the optimizers for each layer\n",
    "optimizer_dense1 = AdamOptimizer(dense_layer1, learning_rate=learning_rate)\n",
    "optimizer_bn1 = AdamOptimizer(batch_norm1, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense2 = AdamOptimizer(dense_layer2, learning_rate=learning_rate)\n",
    "optimizer_bn2 = AdamOptimizer(batch_norm2, learning_rate=learning_rate)\n",
    "\n",
    "# Define metrics storage for visualization\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "val_macro_f1s = []\n",
    "\n",
    "# Define a training function with Batch Normalization, Dropout, and a hidden layer\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Set training mode\n",
    "        batch_norm1.set_training_mode(True)\n",
    "        dropout1.set_training_mode(True)\n",
    "        batch_norm2.set_training_mode(True)\n",
    "        dropout2.set_training_mode(True)\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in trainloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu = relu.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_bn2)\n",
    "\n",
    "            exp_outputs = np.exp(outputs_dropout2 - np.max(outputs_dropout2, axis=1, keepdims=True))\n",
    "            probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "            one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "            loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "            total_train_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            d_out = (probs - one_hot_labels) / labels.shape[0]\n",
    "            d_dropout2 = dropout2.backward(d_out)\n",
    "            d_bn2 = batch_norm2.backward(d_dropout2, learning_rate)\n",
    "            d_dense2 = dense_layer2.backward(d_bn2)\n",
    "\n",
    "            d_dropout1 = dropout1.backward(d_dense2)\n",
    "            d_relu = relu.backward(d_dropout1)\n",
    "            d_bn1 = batch_norm1.backward(d_relu, learning_rate)\n",
    "            dense_layer1.backward(d_bn1)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer_dense1.update()\n",
    "            optimizer_bn1.update()\n",
    "            optimizer_dense2.update()\n",
    "            optimizer_bn2.update()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            predicted = np.argmax(probs, axis=1)\n",
    "            total_train_correct += np.sum(predicted == labels.numpy())\n",
    "            total_train_samples += labels.shape[0]\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "        train_losses.append(total_train_loss / len(trainloader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        batch_norm1.set_training_mode(False)\n",
    "        dropout1.set_training_mode(False)\n",
    "        batch_norm2.set_training_mode(False)\n",
    "        dropout2.set_training_mode(False)\n",
    "\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images = images.view(images.shape[0], -1).numpy()\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                outputs_dense1 = dense_layer1.forward(images)\n",
    "                outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "                outputs_relu = relu.forward(outputs_bn1)\n",
    "                outputs_dropout1 = dropout1.forward(outputs_relu)\n",
    "\n",
    "                outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "                outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "                outputs_dropout2 = dropout2.forward(outputs_bn2)\n",
    "\n",
    "                exp_outputs = np.exp(outputs_dropout2 - np.max(outputs_dropout2, axis=1, keepdims=True))\n",
    "                probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "                one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "                loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "                val_loss += loss\n",
    "\n",
    "                predicted = np.argmax(probs, axis=1)\n",
    "                val_correct += np.sum(predicted == labels.numpy())\n",
    "                val_total += labels.size(0)\n",
    "                all_preds.extend(predicted)\n",
    "                all_labels.extend(labels.numpy())\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        val_losses.append(val_loss / len(valloader))\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_macro_f1s.append(val_macro_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%, Val Macro-F1: {val_macro_f1s[-1]:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train(epochs=100)\n",
    "\n",
    "# Test function to evaluate on the test set\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_norm1.set_training_mode(False)\n",
    "    dropout1.set_training_mode(False)\n",
    "    batch_norm2.set_training_mode(False)\n",
    "    dropout2.set_training_mode(False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()\n",
    "\n",
    "            # Forward pass for testing\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu = relu.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_bn2)\n",
    "\n",
    "            predicted = np.argmax(outputs_dropout2, axis=1)\n",
    "            correct += np.sum(predicted == labels.numpy())\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test()\n",
    "\n",
    "# Plotting the metrics for 100 epochs\n",
    "epochs_range = range(1, 101)  # Now covers 1 to 100 epochs\n",
    "plt.figure(figsize=(14, 8))   # Optional: Increase figure width if desired\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation macro-F1 score\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs_range, val_macro_f1s, label=\"Validation Macro-F1 Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Macro-F1 Score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the DenseLayer, BatchNormalization, ReLU, Dropout, and AdamOptimizer classes as discussed above\n",
    "\n",
    "# Define transformations and load the dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load training and test data\n",
    "trainset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "# Split trainset into training and validation sets (e.g., 90% train, 10% validation)\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Set up data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model layers and optimizers\n",
    "input_dim = 28 * 28  # FashionMNIST images are 28x28\n",
    "hidden_dim = 128     # Hidden layer dimension\n",
    "output_dim = 10      # 10 classes for classification\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define the layers\n",
    "dense_layer1 = DenseLayer(input_dim=input_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm1 = BatchNormalization(dim=hidden_dim)\n",
    "relu1 = ReLU()  # Adding ReLU activation for first hidden layer\n",
    "dropout1 = Dropout(dropout_rate=0.5)  # Dropout for first hidden layer\n",
    "\n",
    "# Second hidden layer\n",
    "dense_layer2 = DenseLayer(input_dim=hidden_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm2 = BatchNormalization(dim=hidden_dim)\n",
    "relu2 = ReLU()  # Adding ReLU activation for second hidden layer\n",
    "dropout2 = Dropout(dropout_rate=0.5)  # Dropout for second hidden layer\n",
    "\n",
    "# Output layer\n",
    "dense_layer3 = DenseLayer(input_dim=hidden_dim, output_dim=output_dim, gradient_clip_value=1.0)\n",
    "batch_norm3 = BatchNormalization(dim=output_dim)\n",
    "dropout3 = Dropout(dropout_rate=0.5)  # Dropout for output layer\n",
    "\n",
    "# Define the optimizers for each layer\n",
    "optimizer_dense1 = AdamOptimizer(dense_layer1, learning_rate=learning_rate)\n",
    "optimizer_bn1 = AdamOptimizer(batch_norm1, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense2 = AdamOptimizer(dense_layer2, learning_rate=learning_rate)\n",
    "optimizer_bn2 = AdamOptimizer(batch_norm2, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense3 = AdamOptimizer(dense_layer3, learning_rate=learning_rate)\n",
    "optimizer_bn3 = AdamOptimizer(batch_norm3, learning_rate=learning_rate)\n",
    "\n",
    "# Define metrics storage for visualization\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "val_macro_f1s = []\n",
    "\n",
    "# Define a training function with Batch Normalization, Dropout, and hidden layers\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Set training mode\n",
    "        batch_norm1.set_training_mode(True)\n",
    "        dropout1.set_training_mode(True)\n",
    "        batch_norm2.set_training_mode(True)\n",
    "        dropout2.set_training_mode(True)\n",
    "        batch_norm3.set_training_mode(True)\n",
    "        dropout3.set_training_mode(True)\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in trainloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_bn3)\n",
    "\n",
    "            exp_outputs = np.exp(outputs_dropout3 - np.max(outputs_dropout3, axis=1, keepdims=True))\n",
    "            probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "            one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "            loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "            total_train_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            d_out = (probs - one_hot_labels) / labels.shape[0]\n",
    "            d_dropout3 = dropout3.backward(d_out)\n",
    "            d_bn3 = batch_norm3.backward(d_dropout3, learning_rate)\n",
    "            d_dense3 = dense_layer3.backward(d_bn3)\n",
    "\n",
    "            d_dropout2 = dropout2.backward(d_dense3)\n",
    "            d_relu2 = relu2.backward(d_dropout2)\n",
    "            d_bn2 = batch_norm2.backward(d_relu2, learning_rate)\n",
    "            d_dense2 = dense_layer2.backward(d_bn2)\n",
    "\n",
    "            d_dropout1 = dropout1.backward(d_dense2)\n",
    "            d_relu1 = relu1.backward(d_dropout1)\n",
    "            d_bn1 = batch_norm1.backward(d_relu1, learning_rate)\n",
    "            dense_layer1.backward(d_bn1)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer_dense1.update()\n",
    "            optimizer_bn1.update()\n",
    "            optimizer_dense2.update()\n",
    "            optimizer_bn2.update()\n",
    "            optimizer_dense3.update()\n",
    "            optimizer_bn3.update()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            predicted = np.argmax(probs, axis=1)\n",
    "            total_train_correct += np.sum(predicted == labels.numpy())\n",
    "            total_train_samples += labels.shape[0]\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "        train_losses.append(total_train_loss / len(trainloader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        batch_norm1.set_training_mode(False)\n",
    "        dropout1.set_training_mode(False)\n",
    "        batch_norm2.set_training_mode(False)\n",
    "        dropout2.set_training_mode(False)\n",
    "        batch_norm3.set_training_mode(False)\n",
    "        dropout3.set_training_mode(False)\n",
    "\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images = images.view(images.shape[0], -1).numpy()\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                outputs_dense1 = dense_layer1.forward(images)\n",
    "                outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "                outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "                outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "                outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "                outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "                outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "                outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "                outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "                outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "                outputs_dropout3 = dropout3.forward(outputs_bn3)\n",
    "\n",
    "                exp_outputs = np.exp(outputs_dropout3 - np.max(outputs_dropout3, axis=1, keepdims=True))\n",
    "                probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "                one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "                loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "                val_loss += loss\n",
    "\n",
    "                predicted = np.argmax(probs, axis=1)\n",
    "                val_correct += np.sum(predicted == labels.numpy())\n",
    "                val_total += labels.size(0)\n",
    "                all_preds.extend(predicted)\n",
    "                all_labels.extend(labels.numpy())\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        val_losses.append(val_loss / len(valloader))\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_macro_f1s.append(val_macro_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%, Val Macro-F1: {val_macro_f1s[-1]:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train(epochs=100)\n",
    "\n",
    "# Test function to evaluate on the test set\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_norm1.set_training_mode(False)\n",
    "    dropout1.set_training_mode(False)\n",
    "    batch_norm2.set_training_mode(False)\n",
    "    dropout2.set_training_mode(False)\n",
    "    batch_norm3.set_training_mode(False)\n",
    "    dropout3.set_training_mode(False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()\n",
    "\n",
    "            # Forward pass for testing\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_bn3)\n",
    "\n",
    "            predicted = np.argmax(outputs_dropout3, axis=1)\n",
    "            correct += np.sum(predicted == labels.numpy())\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test()\n",
    "\n",
    "# Plotting the metrics for 100 epochs\n",
    "epochs_range = range(1, 101)  # Now covers 1 to 100 epochs\n",
    "plt.figure(figsize=(14, 8))   # Optional: Increase figure width if desired\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation macro-F1 score\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs_range, val_macro_f1s, label=\"Validation Macro-F1 Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Macro-F1 Score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the DenseLayer, BatchNormalization, ReLU, Dropout, and AdamOptimizer classes as discussed above\n",
    "\n",
    "# Define transformations and load the dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load training and test data\n",
    "trainset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "# Split trainset into training and validation sets (e.g., 90% train, 10% validation)\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Set up data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model layers and optimizers\n",
    "input_dim = 28 * 28  # FashionMNIST images are 28x28\n",
    "hidden_dim = 128     # Hidden layer dimension\n",
    "output_dim = 10      # 10 classes for classification\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Define the layers\n",
    "dense_layer1 = DenseLayer(input_dim=input_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm1 = BatchNormalization(dim=hidden_dim)\n",
    "relu1 = ReLU()  # Adding ReLU activation for first hidden layer\n",
    "dropout1 = Dropout(dropout_rate=0.5)  # Dropout for first hidden layer\n",
    "\n",
    "# Second hidden layer\n",
    "dense_layer2 = DenseLayer(input_dim=hidden_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm2 = BatchNormalization(dim=hidden_dim)\n",
    "relu2 = ReLU()  # Adding ReLU activation for second hidden layer\n",
    "dropout2 = Dropout(dropout_rate=0.5)  # Dropout for second hidden layer\n",
    "\n",
    "# Output layer\n",
    "dense_layer3 = DenseLayer(input_dim=hidden_dim, output_dim=output_dim, gradient_clip_value=1.0)\n",
    "batch_norm3 = BatchNormalization(dim=output_dim)\n",
    "dropout3 = Dropout(dropout_rate=0.5)  # Dropout for output layer\n",
    "\n",
    "# Define the optimizers for each layer\n",
    "optimizer_dense1 = AdamOptimizer(dense_layer1, learning_rate=learning_rate)\n",
    "optimizer_bn1 = AdamOptimizer(batch_norm1, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense2 = AdamOptimizer(dense_layer2, learning_rate=learning_rate)\n",
    "optimizer_bn2 = AdamOptimizer(batch_norm2, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense3 = AdamOptimizer(dense_layer3, learning_rate=learning_rate)\n",
    "optimizer_bn3 = AdamOptimizer(batch_norm3, learning_rate=learning_rate)\n",
    "\n",
    "# Define metrics storage for visualization\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "val_macro_f1s = []\n",
    "\n",
    "# Define a training function with Batch Normalization, Dropout, and hidden layers\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Set training mode\n",
    "        batch_norm1.set_training_mode(True)\n",
    "        dropout1.set_training_mode(True)\n",
    "        batch_norm2.set_training_mode(True)\n",
    "        dropout2.set_training_mode(True)\n",
    "        batch_norm3.set_training_mode(True)\n",
    "        dropout3.set_training_mode(True)\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in trainloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_bn3)\n",
    "\n",
    "            exp_outputs = np.exp(outputs_dropout3 - np.max(outputs_dropout3, axis=1, keepdims=True))\n",
    "            probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "            one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "            loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "            total_train_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            d_out = (probs - one_hot_labels) / labels.shape[0]\n",
    "            d_dropout3 = dropout3.backward(d_out)\n",
    "            d_bn3 = batch_norm3.backward(d_dropout3, learning_rate)\n",
    "            d_dense3 = dense_layer3.backward(d_bn3)\n",
    "\n",
    "            d_dropout2 = dropout2.backward(d_dense3)\n",
    "            d_relu2 = relu2.backward(d_dropout2)\n",
    "            d_bn2 = batch_norm2.backward(d_relu2, learning_rate)\n",
    "            d_dense2 = dense_layer2.backward(d_bn2)\n",
    "\n",
    "            d_dropout1 = dropout1.backward(d_dense2)\n",
    "            d_relu1 = relu1.backward(d_dropout1)\n",
    "            d_bn1 = batch_norm1.backward(d_relu1, learning_rate)\n",
    "            dense_layer1.backward(d_bn1)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer_dense1.update()\n",
    "            optimizer_bn1.update()\n",
    "            optimizer_dense2.update()\n",
    "            optimizer_bn2.update()\n",
    "            optimizer_dense3.update()\n",
    "            optimizer_bn3.update()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            predicted = np.argmax(probs, axis=1)\n",
    "            total_train_correct += np.sum(predicted == labels.numpy())\n",
    "            total_train_samples += labels.shape[0]\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "        train_losses.append(total_train_loss / len(trainloader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        batch_norm1.set_training_mode(False)\n",
    "        dropout1.set_training_mode(False)\n",
    "        batch_norm2.set_training_mode(False)\n",
    "        dropout2.set_training_mode(False)\n",
    "        batch_norm3.set_training_mode(False)\n",
    "        dropout3.set_training_mode(False)\n",
    "\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images = images.view(images.shape[0], -1).numpy()\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                outputs_dense1 = dense_layer1.forward(images)\n",
    "                outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "                outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "                outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "                outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "                outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "                outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "                outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "                outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "                outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "                outputs_dropout3 = dropout3.forward(outputs_bn3)\n",
    "\n",
    "                exp_outputs = np.exp(outputs_dropout3 - np.max(outputs_dropout3, axis=1, keepdims=True))\n",
    "                probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "                one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "                loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "                val_loss += loss\n",
    "\n",
    "                predicted = np.argmax(probs, axis=1)\n",
    "                val_correct += np.sum(predicted == labels.numpy())\n",
    "                val_total += labels.size(0)\n",
    "                all_preds.extend(predicted)\n",
    "                all_labels.extend(labels.numpy())\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        val_losses.append(val_loss / len(valloader))\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_macro_f1s.append(val_macro_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%, Val Macro-F1: {val_macro_f1s[-1]:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train(epochs=100)\n",
    "\n",
    "# Test function to evaluate on the test set\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_norm1.set_training_mode(False)\n",
    "    dropout1.set_training_mode(False)\n",
    "    batch_norm2.set_training_mode(False)\n",
    "    dropout2.set_training_mode(False)\n",
    "    batch_norm3.set_training_mode(False)\n",
    "    dropout3.set_training_mode(False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()\n",
    "\n",
    "            # Forward pass for testing\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_bn3)\n",
    "\n",
    "            predicted = np.argmax(outputs_dropout3, axis=1)\n",
    "            correct += np.sum(predicted == labels.numpy())\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test()\n",
    "\n",
    "# Plotting the metrics for 100 epochs\n",
    "epochs_range = range(1, 101)  # Now covers 1 to 100 epochs\n",
    "plt.figure(figsize=(14, 8))   # Optional: Increase figure width if desired\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation macro-F1 score\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs_range, val_macro_f1s, label=\"Validation Macro-F1 Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Macro-F1 Score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the DenseLayer, BatchNormalization, ReLU, Dropout, and AdamOptimizer classes as discussed above\n",
    "\n",
    "# Define transformations and load the dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load training and test data\n",
    "trainset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "# Split trainset into training and validation sets (e.g., 90% train, 10% validation)\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Set up data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model layers and optimizers\n",
    "input_dim = 28 * 28  # FashionMNIST images are 28x28\n",
    "hidden_dim = 128     # Hidden layer dimension\n",
    "output_dim = 10      # 10 classes for classification\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Define the layers\n",
    "dense_layer1 = DenseLayer(input_dim=input_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm1 = BatchNormalization(dim=hidden_dim)\n",
    "relu1 = ReLU()  # Adding ReLU activation for first hidden layer\n",
    "dropout1 = Dropout(dropout_rate=0.5)  # Dropout for first hidden layer\n",
    "\n",
    "# Second hidden layer\n",
    "dense_layer2 = DenseLayer(input_dim=hidden_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm2 = BatchNormalization(dim=hidden_dim)\n",
    "relu2 = ReLU()  # Adding ReLU activation for second hidden layer\n",
    "dropout2 = Dropout(dropout_rate=0.5)  # Dropout for second hidden layer\n",
    "\n",
    "# Output layer\n",
    "dense_layer3 = DenseLayer(input_dim=hidden_dim, output_dim=output_dim, gradient_clip_value=1.0)\n",
    "batch_norm3 = BatchNormalization(dim=output_dim)\n",
    "dropout3 = Dropout(dropout_rate=0.5)  # Dropout for output layer\n",
    "\n",
    "# Define the optimizers for each layer\n",
    "optimizer_dense1 = AdamOptimizer(dense_layer1, learning_rate=learning_rate)\n",
    "optimizer_bn1 = AdamOptimizer(batch_norm1, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense2 = AdamOptimizer(dense_layer2, learning_rate=learning_rate)\n",
    "optimizer_bn2 = AdamOptimizer(batch_norm2, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense3 = AdamOptimizer(dense_layer3, learning_rate=learning_rate)\n",
    "optimizer_bn3 = AdamOptimizer(batch_norm3, learning_rate=learning_rate)\n",
    "\n",
    "# Define metrics storage for visualization\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "val_macro_f1s = []\n",
    "\n",
    "# Define a training function with Batch Normalization, Dropout, and hidden layers\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Set training mode\n",
    "        batch_norm1.set_training_mode(True)\n",
    "        dropout1.set_training_mode(True)\n",
    "        batch_norm2.set_training_mode(True)\n",
    "        dropout2.set_training_mode(True)\n",
    "        batch_norm3.set_training_mode(True)\n",
    "        dropout3.set_training_mode(True)\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in trainloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_bn3)\n",
    "\n",
    "            exp_outputs = np.exp(outputs_dropout3 - np.max(outputs_dropout3, axis=1, keepdims=True))\n",
    "            probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "            one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "            loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "            total_train_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            d_out = (probs - one_hot_labels) / labels.shape[0]\n",
    "            d_dropout3 = dropout3.backward(d_out)\n",
    "            d_bn3 = batch_norm3.backward(d_dropout3, learning_rate)\n",
    "            d_dense3 = dense_layer3.backward(d_bn3)\n",
    "\n",
    "            d_dropout2 = dropout2.backward(d_dense3)\n",
    "            d_relu2 = relu2.backward(d_dropout2)\n",
    "            d_bn2 = batch_norm2.backward(d_relu2, learning_rate)\n",
    "            d_dense2 = dense_layer2.backward(d_bn2)\n",
    "\n",
    "            d_dropout1 = dropout1.backward(d_dense2)\n",
    "            d_relu1 = relu1.backward(d_dropout1)\n",
    "            d_bn1 = batch_norm1.backward(d_relu1, learning_rate)\n",
    "            dense_layer1.backward(d_bn1)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer_dense1.update()\n",
    "            optimizer_bn1.update()\n",
    "            optimizer_dense2.update()\n",
    "            optimizer_bn2.update()\n",
    "            optimizer_dense3.update()\n",
    "            optimizer_bn3.update()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            predicted = np.argmax(probs, axis=1)\n",
    "            total_train_correct += np.sum(predicted == labels.numpy())\n",
    "            total_train_samples += labels.shape[0]\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "        train_losses.append(total_train_loss / len(trainloader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        batch_norm1.set_training_mode(False)\n",
    "        dropout1.set_training_mode(False)\n",
    "        batch_norm2.set_training_mode(False)\n",
    "        dropout2.set_training_mode(False)\n",
    "        batch_norm3.set_training_mode(False)\n",
    "        dropout3.set_training_mode(False)\n",
    "\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images = images.view(images.shape[0], -1).numpy()\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                outputs_dense1 = dense_layer1.forward(images)\n",
    "                outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "                outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "                outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "                outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "                outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "                outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "                outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "                outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "                outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "                outputs_dropout3 = dropout3.forward(outputs_bn3)\n",
    "\n",
    "                exp_outputs = np.exp(outputs_dropout3 - np.max(outputs_dropout3, axis=1, keepdims=True))\n",
    "                probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "                one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "                loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "                val_loss += loss\n",
    "\n",
    "                predicted = np.argmax(probs, axis=1)\n",
    "                val_correct += np.sum(predicted == labels.numpy())\n",
    "                val_total += labels.size(0)\n",
    "                all_preds.extend(predicted)\n",
    "                all_labels.extend(labels.numpy())\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        val_losses.append(val_loss / len(valloader))\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_macro_f1s.append(val_macro_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%, Val Macro-F1: {val_macro_f1s[-1]:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train(epochs=100)\n",
    "\n",
    "# Test function to evaluate on the test set\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_norm1.set_training_mode(False)\n",
    "    dropout1.set_training_mode(False)\n",
    "    batch_norm2.set_training_mode(False)\n",
    "    dropout2.set_training_mode(False)\n",
    "    batch_norm3.set_training_mode(False)\n",
    "    dropout3.set_training_mode(False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()\n",
    "\n",
    "            # Forward pass for testing\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_bn3)\n",
    "\n",
    "            predicted = np.argmax(outputs_dropout3, axis=1)\n",
    "            correct += np.sum(predicted == labels.numpy())\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test()\n",
    "\n",
    "# Plotting the metrics for 100 epochs\n",
    "epochs_range = range(1, 101)  # Now covers 1 to 100 epochs\n",
    "plt.figure(figsize=(14, 8))   # Optional: Increase figure width if desired\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation macro-F1 score\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs_range, val_macro_f1s, label=\"Validation Macro-F1 Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Macro-F1 Score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the DenseLayer, BatchNormalization, ReLU, Dropout, and AdamOptimizer classes as discussed above\n",
    "\n",
    "# Define transformations and load the dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load training and test data\n",
    "trainset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "# Split trainset into training and validation sets (e.g., 90% train, 10% validation)\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Set up data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model layers and optimizers\n",
    "input_dim = 28 * 28  # FashionMNIST images are 28x28\n",
    "hidden_dim = 128     # Hidden layer dimension\n",
    "output_dim = 10      # 10 classes for classification\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define the layers\n",
    "dense_layer1 = DenseLayer(input_dim=input_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm1 = BatchNormalization(dim=hidden_dim)\n",
    "relu1 = ReLU()  # Adding ReLU activation for first hidden layer\n",
    "dropout1 = Dropout(dropout_rate=0.5)  # Dropout for first hidden layer\n",
    "\n",
    "# Second hidden layer\n",
    "dense_layer2 = DenseLayer(input_dim=hidden_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm2 = BatchNormalization(dim=hidden_dim)\n",
    "relu2 = ReLU()  # Adding ReLU activation for second hidden layer\n",
    "dropout2 = Dropout(dropout_rate=0.5)  # Dropout for second hidden layer\n",
    "\n",
    "# Third hidden layer\n",
    "dense_layer3 = DenseLayer(input_dim=hidden_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm3 = BatchNormalization(dim=hidden_dim)\n",
    "relu3 = ReLU()  # Adding ReLU activation for third hidden layer\n",
    "dropout3 = Dropout(dropout_rate=0.5)  # Dropout for third hidden layer\n",
    "\n",
    "# Output layer\n",
    "dense_layer4 = DenseLayer(input_dim=hidden_dim, output_dim=output_dim, gradient_clip_value=1.0)\n",
    "batch_norm4 = BatchNormalization(dim=output_dim)\n",
    "dropout4 = Dropout(dropout_rate=0.5)  # Dropout for output layer\n",
    "\n",
    "# Define the optimizers for each layer\n",
    "optimizer_dense1 = AdamOptimizer(dense_layer1, learning_rate=learning_rate)\n",
    "optimizer_bn1 = AdamOptimizer(batch_norm1, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense2 = AdamOptimizer(dense_layer2, learning_rate=learning_rate)\n",
    "optimizer_bn2 = AdamOptimizer(batch_norm2, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense3 = AdamOptimizer(dense_layer3, learning_rate=learning_rate)\n",
    "optimizer_bn3 = AdamOptimizer(batch_norm3, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense4 = AdamOptimizer(dense_layer4, learning_rate=learning_rate)\n",
    "optimizer_bn4 = AdamOptimizer(batch_norm4, learning_rate=learning_rate)\n",
    "\n",
    "# Define metrics storage for visualization\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "val_macro_f1s = []\n",
    "\n",
    "# Define a training function with Batch Normalization, Dropout, and hidden layers\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Set training mode\n",
    "        batch_norm1.set_training_mode(True)\n",
    "        dropout1.set_training_mode(True)\n",
    "        batch_norm2.set_training_mode(True)\n",
    "        dropout2.set_training_mode(True)\n",
    "        batch_norm3.set_training_mode(True)\n",
    "        dropout3.set_training_mode(True)\n",
    "        batch_norm4.set_training_mode(True)\n",
    "        dropout4.set_training_mode(True)\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in trainloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_relu3 = relu3.forward(outputs_bn3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_relu3)\n",
    "\n",
    "            outputs_dense4 = dense_layer4.forward(outputs_dropout3)\n",
    "            outputs_bn4 = batch_norm4.forward(outputs_dense4)\n",
    "            outputs_dropout4 = dropout4.forward(outputs_bn4)\n",
    "\n",
    "            exp_outputs = np.exp(outputs_dropout4 - np.max(outputs_dropout4, axis=1, keepdims=True))\n",
    "            probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "            one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "            loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "            total_train_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            d_out = (probs - one_hot_labels) / labels.shape[0]\n",
    "            d_dropout4 = dropout4.backward(d_out)\n",
    "            d_bn4 = batch_norm4.backward(d_dropout4, learning_rate)\n",
    "            d_dense4 = dense_layer4.backward(d_bn4)\n",
    "\n",
    "            d_dropout3 = dropout3.backward(d_dense4)\n",
    "            d_relu3 = relu3.backward(d_dropout3)\n",
    "            d_bn3 = batch_norm3.backward(d_relu3, learning_rate)\n",
    "            d_dense3 = dense_layer3.backward(d_bn3)\n",
    "\n",
    "            d_dropout2 = dropout2.backward(d_dense3)\n",
    "            d_relu2 = relu2.backward(d_dropout2)\n",
    "            d_bn2 = batch_norm2.backward(d_relu2, learning_rate)\n",
    "            d_dense2 = dense_layer2.backward(d_bn2)\n",
    "\n",
    "            d_dropout1 = dropout1.backward(d_dense2)\n",
    "            d_relu1 = relu1.backward(d_dropout1)\n",
    "            d_bn1 = batch_norm1.backward(d_relu1, learning_rate)\n",
    "            dense_layer1.backward(d_bn1)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer_dense1.update()\n",
    "            optimizer_bn1.update()\n",
    "            optimizer_dense2.update()\n",
    "            optimizer_bn2.update()\n",
    "            optimizer_dense3.update()\n",
    "            optimizer_bn3.update()\n",
    "            optimizer_dense4.update()\n",
    "            optimizer_bn4.update()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            predicted = np.argmax(probs, axis=1)\n",
    "            total_train_correct += np.sum(predicted == labels.numpy())\n",
    "            total_train_samples += labels.shape[0]\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "        train_losses.append(total_train_loss / len(trainloader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        batch_norm1.set_training_mode(False)\n",
    "        dropout1.set_training_mode(False)\n",
    "        batch_norm2.set_training_mode(False)\n",
    "        dropout2.set_training_mode(False)\n",
    "        batch_norm3.set_training_mode(False)\n",
    "        dropout3.set_training_mode(False)\n",
    "        batch_norm4.set_training_mode(False)\n",
    "        dropout4.set_training_mode(False)\n",
    "\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images = images.view(images.shape[0], -1).numpy()\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                outputs_dense1 = dense_layer1.forward(images)\n",
    "                outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "                outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "                outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "                outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "                outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "                outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "                outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "                outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "                outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "                outputs_relu3 = relu3.forward(outputs_bn3)\n",
    "                outputs_dropout3 = dropout3.forward(outputs_relu3)\n",
    "\n",
    "                outputs_dense4 = dense_layer4.forward(outputs_dropout3)\n",
    "                outputs_bn4 = batch_norm4.forward(outputs_dense4)\n",
    "                outputs_dropout4 = dropout4.forward(outputs_bn4)\n",
    "\n",
    "                exp_outputs = np.exp(outputs_dropout4 - np.max(outputs_dropout4, axis=1, keepdims=True))\n",
    "                probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "                one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "                loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "                val_loss += loss\n",
    "\n",
    "                predicted = np.argmax(probs, axis=1)\n",
    "                val_correct += np.sum(predicted == labels.numpy())\n",
    "                val_total += labels.size(0)\n",
    "                all_preds.extend(predicted)\n",
    "                all_labels.extend(labels.numpy())\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        val_losses.append(val_loss / len(valloader))\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_macro_f1s.append(val_macro_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%, Val Macro-F1: {val_macro_f1s[-1]:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train(epochs=100)\n",
    "\n",
    "# Test function to evaluate on the test set\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_norm1.set_training_mode(False)\n",
    "    dropout1.set_training_mode(False)\n",
    "    batch_norm2.set_training_mode(False)\n",
    "    dropout2.set_training_mode(False)\n",
    "    batch_norm3.set_training_mode(False)\n",
    "    dropout3.set_training_mode(False)\n",
    "    batch_norm4.set_training_mode(False)\n",
    "    dropout4.set_training_mode(False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()\n",
    "\n",
    "            # Forward pass for testing\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_relu3 = relu3.forward(outputs_bn3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_relu3)\n",
    "\n",
    "            outputs_dense4 = dense_layer4.forward(outputs_dropout3)\n",
    "            outputs_bn4 = batch_norm4.forward(outputs_dense4)\n",
    "            outputs_dropout4 = dropout4.forward(outputs_bn4)\n",
    "\n",
    "            predicted = np.argmax(outputs_dropout4, axis=1)\n",
    "            correct += np.sum(predicted == labels.numpy())\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test()\n",
    "\n",
    "# Plotting the metrics for 100 epochs\n",
    "epochs_range = range(1, 101)  # Now covers 1 to 100 epochs\n",
    "plt.figure(figsize=(14, 8))   # Optional: Increase figure width if desired\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation macro-F1 score\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs_range, val_macro_f1s, label=\"Validation Macro-F1 Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Macro-F1 Score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the DenseLayer, BatchNormalization, ReLU, Dropout, and AdamOptimizer classes as discussed above\n",
    "\n",
    "# Define transformations and load the dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load training and test data\n",
    "trainset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "# Split trainset into training and validation sets (e.g., 90% train, 10% validation)\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Set up data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model layers and optimizers\n",
    "input_dim = 28 * 28  # FashionMNIST images are 28x28\n",
    "hidden_dim = 128     # Hidden layer dimension\n",
    "output_dim = 10      # 10 classes for classification\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Define the layers\n",
    "dense_layer1 = DenseLayer(input_dim=input_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm1 = BatchNormalization(dim=hidden_dim)\n",
    "relu1 = ReLU()  # Adding ReLU activation for first hidden layer\n",
    "dropout1 = Dropout(dropout_rate=0.5)  # Dropout for first hidden layer\n",
    "\n",
    "# Second hidden layer\n",
    "dense_layer2 = DenseLayer(input_dim=hidden_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm2 = BatchNormalization(dim=hidden_dim)\n",
    "relu2 = ReLU()  # Adding ReLU activation for second hidden layer\n",
    "dropout2 = Dropout(dropout_rate=0.5)  # Dropout for second hidden layer\n",
    "\n",
    "# Third hidden layer\n",
    "dense_layer3 = DenseLayer(input_dim=hidden_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm3 = BatchNormalization(dim=hidden_dim)\n",
    "relu3 = ReLU()  # Adding ReLU activation for third hidden layer\n",
    "dropout3 = Dropout(dropout_rate=0.5)  # Dropout for third hidden layer\n",
    "\n",
    "# Output layer\n",
    "dense_layer4 = DenseLayer(input_dim=hidden_dim, output_dim=output_dim, gradient_clip_value=1.0)\n",
    "batch_norm4 = BatchNormalization(dim=output_dim)\n",
    "dropout4 = Dropout(dropout_rate=0.5)  # Dropout for output layer\n",
    "\n",
    "# Define the optimizers for each layer\n",
    "optimizer_dense1 = AdamOptimizer(dense_layer1, learning_rate=learning_rate)\n",
    "optimizer_bn1 = AdamOptimizer(batch_norm1, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense2 = AdamOptimizer(dense_layer2, learning_rate=learning_rate)\n",
    "optimizer_bn2 = AdamOptimizer(batch_norm2, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense3 = AdamOptimizer(dense_layer3, learning_rate=learning_rate)\n",
    "optimizer_bn3 = AdamOptimizer(batch_norm3, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense4 = AdamOptimizer(dense_layer4, learning_rate=learning_rate)\n",
    "optimizer_bn4 = AdamOptimizer(batch_norm4, learning_rate=learning_rate)\n",
    "\n",
    "# Define metrics storage for visualization\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "val_macro_f1s = []\n",
    "\n",
    "# Define a training function with Batch Normalization, Dropout, and hidden layers\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Set training mode\n",
    "        batch_norm1.set_training_mode(True)\n",
    "        dropout1.set_training_mode(True)\n",
    "        batch_norm2.set_training_mode(True)\n",
    "        dropout2.set_training_mode(True)\n",
    "        batch_norm3.set_training_mode(True)\n",
    "        dropout3.set_training_mode(True)\n",
    "        batch_norm4.set_training_mode(True)\n",
    "        dropout4.set_training_mode(True)\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in trainloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_relu3 = relu3.forward(outputs_bn3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_relu3)\n",
    "\n",
    "            outputs_dense4 = dense_layer4.forward(outputs_dropout3)\n",
    "            outputs_bn4 = batch_norm4.forward(outputs_dense4)\n",
    "            outputs_dropout4 = dropout4.forward(outputs_bn4)\n",
    "\n",
    "            exp_outputs = np.exp(outputs_dropout4 - np.max(outputs_dropout4, axis=1, keepdims=True))\n",
    "            probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "            one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "            loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "            total_train_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            d_out = (probs - one_hot_labels) / labels.shape[0]\n",
    "            d_dropout4 = dropout4.backward(d_out)\n",
    "            d_bn4 = batch_norm4.backward(d_dropout4, learning_rate)\n",
    "            d_dense4 = dense_layer4.backward(d_bn4)\n",
    "\n",
    "            d_dropout3 = dropout3.backward(d_dense4)\n",
    "            d_relu3 = relu3.backward(d_dropout3)\n",
    "            d_bn3 = batch_norm3.backward(d_relu3, learning_rate)\n",
    "            d_dense3 = dense_layer3.backward(d_bn3)\n",
    "\n",
    "            d_dropout2 = dropout2.backward(d_dense3)\n",
    "            d_relu2 = relu2.backward(d_dropout2)\n",
    "            d_bn2 = batch_norm2.backward(d_relu2, learning_rate)\n",
    "            d_dense2 = dense_layer2.backward(d_bn2)\n",
    "\n",
    "            d_dropout1 = dropout1.backward(d_dense2)\n",
    "            d_relu1 = relu1.backward(d_dropout1)\n",
    "            d_bn1 = batch_norm1.backward(d_relu1, learning_rate)\n",
    "            dense_layer1.backward(d_bn1)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer_dense1.update()\n",
    "            optimizer_bn1.update()\n",
    "            optimizer_dense2.update()\n",
    "            optimizer_bn2.update()\n",
    "            optimizer_dense3.update()\n",
    "            optimizer_bn3.update()\n",
    "            optimizer_dense4.update()\n",
    "            optimizer_bn4.update()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            predicted = np.argmax(probs, axis=1)\n",
    "            total_train_correct += np.sum(predicted == labels.numpy())\n",
    "            total_train_samples += labels.shape[0]\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "        train_losses.append(total_train_loss / len(trainloader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        batch_norm1.set_training_mode(False)\n",
    "        dropout1.set_training_mode(False)\n",
    "        batch_norm2.set_training_mode(False)\n",
    "        dropout2.set_training_mode(False)\n",
    "        batch_norm3.set_training_mode(False)\n",
    "        dropout3.set_training_mode(False)\n",
    "        batch_norm4.set_training_mode(False)\n",
    "        dropout4.set_training_mode(False)\n",
    "\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images = images.view(images.shape[0], -1).numpy()\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                outputs_dense1 = dense_layer1.forward(images)\n",
    "                outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "                outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "                outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "                outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "                outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "                outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "                outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "                outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "                outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "                outputs_relu3 = relu3.forward(outputs_bn3)\n",
    "                outputs_dropout3 = dropout3.forward(outputs_relu3)\n",
    "\n",
    "                outputs_dense4 = dense_layer4.forward(outputs_dropout3)\n",
    "                outputs_bn4 = batch_norm4.forward(outputs_dense4)\n",
    "                outputs_dropout4 = dropout4.forward(outputs_bn4)\n",
    "\n",
    "                exp_outputs = np.exp(outputs_dropout4 - np.max(outputs_dropout4, axis=1, keepdims=True))\n",
    "                probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "                one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "                loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "                val_loss += loss\n",
    "\n",
    "                predicted = np.argmax(probs, axis=1)\n",
    "                val_correct += np.sum(predicted == labels.numpy())\n",
    "                val_total += labels.size(0)\n",
    "                all_preds.extend(predicted)\n",
    "                all_labels.extend(labels.numpy())\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        val_losses.append(val_loss / len(valloader))\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_macro_f1s.append(val_macro_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%, Val Macro-F1: {val_macro_f1s[-1]:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train(epochs=100)\n",
    "\n",
    "# Test function to evaluate on the test set\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_norm1.set_training_mode(False)\n",
    "    dropout1.set_training_mode(False)\n",
    "    batch_norm2.set_training_mode(False)\n",
    "    dropout2.set_training_mode(False)\n",
    "    batch_norm3.set_training_mode(False)\n",
    "    dropout3.set_training_mode(False)\n",
    "    batch_norm4.set_training_mode(False)\n",
    "    dropout4.set_training_mode(False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()\n",
    "\n",
    "            # Forward pass for testing\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_relu3 = relu3.forward(outputs_bn3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_relu3)\n",
    "\n",
    "            outputs_dense4 = dense_layer4.forward(outputs_dropout3)\n",
    "            outputs_bn4 = batch_norm4.forward(outputs_dense4)\n",
    "            outputs_dropout4 = dropout4.forward(outputs_bn4)\n",
    "\n",
    "            predicted = np.argmax(outputs_dropout4, axis=1)\n",
    "            correct += np.sum(predicted == labels.numpy())\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test()\n",
    "\n",
    "# Plotting the metrics for 100 epochs\n",
    "epochs_range = range(1, 101)  # Now covers 1 to 100 epochs\n",
    "plt.figure(figsize=(14, 8))   # Optional: Increase figure width if desired\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation macro-F1 score\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs_range, val_macro_f1s, label=\"Validation Macro-F1 Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Macro-F1 Score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the DenseLayer, BatchNormalization, ReLU, Dropout, and AdamOptimizer classes as discussed above\n",
    "\n",
    "# Define transformations and load the dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load training and test data\n",
    "trainset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST(root='~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "# Split trainset into training and validation sets (e.g., 90% train, 10% validation)\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Set up data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model layers and optimizers\n",
    "input_dim = 28 * 28  # FashionMNIST images are 28x28\n",
    "hidden_dim = 128     # Hidden layer dimension\n",
    "output_dim = 10      # 10 classes for classification\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Define the layers\n",
    "dense_layer1 = DenseLayer(input_dim=input_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm1 = BatchNormalization(dim=hidden_dim)\n",
    "relu1 = ReLU()  # Adding ReLU activation for first hidden layer\n",
    "dropout1 = Dropout(dropout_rate=0.5)  # Dropout for first hidden layer\n",
    "\n",
    "# Second hidden layer\n",
    "dense_layer2 = DenseLayer(input_dim=hidden_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm2 = BatchNormalization(dim=hidden_dim)\n",
    "relu2 = ReLU()  # Adding ReLU activation for second hidden layer\n",
    "dropout2 = Dropout(dropout_rate=0.5)  # Dropout for second hidden layer\n",
    "\n",
    "# Third hidden layer\n",
    "dense_layer3 = DenseLayer(input_dim=hidden_dim, output_dim=hidden_dim, gradient_clip_value=1.0)\n",
    "batch_norm3 = BatchNormalization(dim=hidden_dim)\n",
    "relu3 = ReLU()  # Adding ReLU activation for third hidden layer\n",
    "dropout3 = Dropout(dropout_rate=0.5)  # Dropout for third hidden layer\n",
    "\n",
    "# Output layer\n",
    "dense_layer4 = DenseLayer(input_dim=hidden_dim, output_dim=output_dim, gradient_clip_value=1.0)\n",
    "batch_norm4 = BatchNormalization(dim=output_dim)\n",
    "dropout4 = Dropout(dropout_rate=0.5)  # Dropout for output layer\n",
    "\n",
    "# Define the optimizers for each layer\n",
    "optimizer_dense1 = AdamOptimizer(dense_layer1, learning_rate=learning_rate)\n",
    "optimizer_bn1 = AdamOptimizer(batch_norm1, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense2 = AdamOptimizer(dense_layer2, learning_rate=learning_rate)\n",
    "optimizer_bn2 = AdamOptimizer(batch_norm2, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense3 = AdamOptimizer(dense_layer3, learning_rate=learning_rate)\n",
    "optimizer_bn3 = AdamOptimizer(batch_norm3, learning_rate=learning_rate)\n",
    "\n",
    "optimizer_dense4 = AdamOptimizer(dense_layer4, learning_rate=learning_rate)\n",
    "optimizer_bn4 = AdamOptimizer(batch_norm4, learning_rate=learning_rate)\n",
    "\n",
    "# Define metrics storage for visualization\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "val_macro_f1s = []\n",
    "\n",
    "# Define a training function with Batch Normalization, Dropout, and hidden layers\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Set training mode\n",
    "        batch_norm1.set_training_mode(True)\n",
    "        dropout1.set_training_mode(True)\n",
    "        batch_norm2.set_training_mode(True)\n",
    "        dropout2.set_training_mode(True)\n",
    "        batch_norm3.set_training_mode(True)\n",
    "        dropout3.set_training_mode(True)\n",
    "        batch_norm4.set_training_mode(True)\n",
    "        dropout4.set_training_mode(True)\n",
    "        \n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels in trainloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_relu3 = relu3.forward(outputs_bn3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_relu3)\n",
    "\n",
    "            outputs_dense4 = dense_layer4.forward(outputs_dropout3)\n",
    "            outputs_bn4 = batch_norm4.forward(outputs_dense4)\n",
    "            outputs_dropout4 = dropout4.forward(outputs_bn4)\n",
    "\n",
    "            exp_outputs = np.exp(outputs_dropout4 - np.max(outputs_dropout4, axis=1, keepdims=True))\n",
    "            probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "            one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "            loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "            total_train_loss += loss\n",
    "\n",
    "            # Backpropagation\n",
    "            d_out = (probs - one_hot_labels) / labels.shape[0]\n",
    "            d_dropout4 = dropout4.backward(d_out)\n",
    "            d_bn4 = batch_norm4.backward(d_dropout4, learning_rate)\n",
    "            d_dense4 = dense_layer4.backward(d_bn4)\n",
    "\n",
    "            d_dropout3 = dropout3.backward(d_dense4)\n",
    "            d_relu3 = relu3.backward(d_dropout3)\n",
    "            d_bn3 = batch_norm3.backward(d_relu3, learning_rate)\n",
    "            d_dense3 = dense_layer3.backward(d_bn3)\n",
    "\n",
    "            d_dropout2 = dropout2.backward(d_dense3)\n",
    "            d_relu2 = relu2.backward(d_dropout2)\n",
    "            d_bn2 = batch_norm2.backward(d_relu2, learning_rate)\n",
    "            d_dense2 = dense_layer2.backward(d_bn2)\n",
    "\n",
    "            d_dropout1 = dropout1.backward(d_dense2)\n",
    "            d_relu1 = relu1.backward(d_dropout1)\n",
    "            d_bn1 = batch_norm1.backward(d_relu1, learning_rate)\n",
    "            dense_layer1.backward(d_bn1)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer_dense1.update()\n",
    "            optimizer_bn1.update()\n",
    "            optimizer_dense2.update()\n",
    "            optimizer_bn2.update()\n",
    "            optimizer_dense3.update()\n",
    "            optimizer_bn3.update()\n",
    "            optimizer_dense4.update()\n",
    "            optimizer_bn4.update()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            predicted = np.argmax(probs, axis=1)\n",
    "            total_train_correct += np.sum(predicted == labels.numpy())\n",
    "            total_train_samples += labels.shape[0]\n",
    "\n",
    "        train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "        train_losses.append(total_train_loss / len(trainloader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        batch_norm1.set_training_mode(False)\n",
    "        dropout1.set_training_mode(False)\n",
    "        batch_norm2.set_training_mode(False)\n",
    "        dropout2.set_training_mode(False)\n",
    "        batch_norm3.set_training_mode(False)\n",
    "        dropout3.set_training_mode(False)\n",
    "        batch_norm4.set_training_mode(False)\n",
    "        dropout4.set_training_mode(False)\n",
    "\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images = images.view(images.shape[0], -1).numpy()\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                outputs_dense1 = dense_layer1.forward(images)\n",
    "                outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "                outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "                outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "                outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "                outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "                outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "                outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "                outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "                outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "                outputs_relu3 = relu3.forward(outputs_bn3)\n",
    "                outputs_dropout3 = dropout3.forward(outputs_relu3)\n",
    "\n",
    "                outputs_dense4 = dense_layer4.forward(outputs_dropout3)\n",
    "                outputs_bn4 = batch_norm4.forward(outputs_dense4)\n",
    "                outputs_dropout4 = dropout4.forward(outputs_bn4)\n",
    "\n",
    "                exp_outputs = np.exp(outputs_dropout4 - np.max(outputs_dropout4, axis=1, keepdims=True))\n",
    "                probs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)\n",
    "\n",
    "                one_hot_labels = np.eye(output_dim)[labels.numpy()]\n",
    "                loss = -np.sum(one_hot_labels * np.log(probs + 1e-8)) / labels.shape[0]\n",
    "                val_loss += loss\n",
    "\n",
    "                predicted = np.argmax(probs, axis=1)\n",
    "                val_correct += np.sum(predicted == labels.numpy())\n",
    "                val_total += labels.size(0)\n",
    "                all_preds.extend(predicted)\n",
    "                all_labels.extend(labels.numpy())\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        \n",
    "        val_losses.append(val_loss / len(valloader))\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_macro_f1s.append(val_macro_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%, Val Macro-F1: {val_macro_f1s[-1]:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train(epochs=100)\n",
    "\n",
    "# Test function to evaluate on the test set\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_norm1.set_training_mode(False)\n",
    "    dropout1.set_training_mode(False)\n",
    "    batch_norm2.set_training_mode(False)\n",
    "    dropout2.set_training_mode(False)\n",
    "    batch_norm3.set_training_mode(False)\n",
    "    dropout3.set_training_mode(False)\n",
    "    batch_norm4.set_training_mode(False)\n",
    "    dropout4.set_training_mode(False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.view(images.shape[0], -1).numpy()\n",
    "\n",
    "            # Forward pass for testing\n",
    "            outputs_dense1 = dense_layer1.forward(images)\n",
    "            outputs_bn1 = batch_norm1.forward(outputs_dense1)\n",
    "            outputs_relu1 = relu1.forward(outputs_bn1)\n",
    "            outputs_dropout1 = dropout1.forward(outputs_relu1)\n",
    "\n",
    "            outputs_dense2 = dense_layer2.forward(outputs_dropout1)\n",
    "            outputs_bn2 = batch_norm2.forward(outputs_dense2)\n",
    "            outputs_relu2 = relu2.forward(outputs_bn2)\n",
    "            outputs_dropout2 = dropout2.forward(outputs_relu2)\n",
    "\n",
    "            outputs_dense3 = dense_layer3.forward(outputs_dropout2)\n",
    "            outputs_bn3 = batch_norm3.forward(outputs_dense3)\n",
    "            outputs_relu3 = relu3.forward(outputs_bn3)\n",
    "            outputs_dropout3 = dropout3.forward(outputs_relu3)\n",
    "\n",
    "            outputs_dense4 = dense_layer4.forward(outputs_dropout3)\n",
    "            outputs_bn4 = batch_norm4.forward(outputs_dense4)\n",
    "            outputs_dropout4 = dropout4.forward(outputs_bn4)\n",
    "\n",
    "            predicted = np.argmax(outputs_dropout4, axis=1)\n",
    "            correct += np.sum(predicted == labels.numpy())\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test()\n",
    "\n",
    "# Plotting the metrics for 100 epochs\n",
    "epochs_range = range(1, 101)  # Now covers 1 to 100 epochs\n",
    "plt.figure(figsize=(14, 8))   # Optional: Increase figure width if desired\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation macro-F1 score\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs_range, val_macro_f1s, label=\"Validation Macro-F1 Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Macro-F1 Score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
