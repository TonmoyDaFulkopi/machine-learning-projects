{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonmoy\\AppData\\Local\\Temp\\ipykernel_5060\\3423721479.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataframe[column].fillna(mode_value, inplace=True)  # Replace NaNs with mode\n",
      "C:\\Users\\tonmoy\\AppData\\Local\\Temp\\ipykernel_5060\\3423721479.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataframe[column].fillna(mode_value, inplace=True)  # Replace NaNs with mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (226980, 30) (226980,)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the CSV file name\n",
    "csv_name = \"csv3.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "dataframe = pd.read_csv(csv_name)\n",
    "\n",
    "# Step 1: Drop duplicates\n",
    "dataframe = dataframe.drop_duplicates()\n",
    "\n",
    "# Step 2: Replace missing values with the mode for each column\n",
    "for column in dataframe.columns:\n",
    "    mode_value = dataframe[column].mode()[0]  # Get the mode of the column\n",
    "    dataframe[column].fillna(mode_value, inplace=True)  # Replace NaNs with mode\n",
    "\n",
    "# Step 3: Identify high cardinality columns (numeric columns with many unique values)\n",
    "# Set a threshold for high cardinality (e.g., more than 50 unique values)\n",
    "high_cardinality_cols = [col for col in dataframe.columns if dataframe[col].nunique() > 50]\n",
    "\n",
    "# Drop high cardinality columns before scaling\n",
    "# dataframe = dataframe.drop(columns=high_cardinality_cols)\n",
    "\n",
    "# Step 4: Define features and target\n",
    "target_column = \"Class\"  # Target column is 'Class'\n",
    "features = dataframe.drop(columns=[target_column])\n",
    "target = dataframe[target_column]\n",
    "\n",
    "# Ensure features is not empty and contains only numeric columns\n",
    "if features.empty:\n",
    "    raise ValueError(\"Features dataframe is empty after dropping high cardinality columns.\")\n",
    "\n",
    "# Check for any non-numeric columns\n",
    "non_numeric_cols = features.select_dtypes(exclude=[np.number]).columns\n",
    "if len(non_numeric_cols) > 0:\n",
    "    raise ValueError(f\"The following columns are non-numeric: {non_numeric_cols}\")\n",
    "\n",
    "# Step 5: Scaling function\n",
    "standardize = True\n",
    "\n",
    "def scale(dataframe, standardize):\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    # Ensure the dataframe is not empty before scaling\n",
    "    if dataframe.empty:\n",
    "        raise ValueError(\"DataFrame is empty, cannot apply scaler.\")\n",
    "    return pd.DataFrame(scaler.fit_transform(dataframe), columns=dataframe.columns)\n",
    "\n",
    "# Scale the features\n",
    "features = scale(features, standardize)\n",
    "\n",
    "# Step 6: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
